{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8d6229-3909-42cf-bff3-5d8a12454022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ea42a8-42b8-4c6a-9bd9-87c81694c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../AI-Agents.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9713093-3cb7-486f-a886-a57b99a893f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list=[]\n",
    "with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in  f.readlines():\n",
    "        str_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a615a1-4256-4470-b643-39fadba6cd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '# **LLM-Agents**\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t\\t随着大语言模型的兴起，AI Agent这一词也随之变得火热，实际上AI Agent已经经历了几个阶段的演变。我们可以先进行一下简短的回顾：\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t\\t$\\\\text{Symbolic Agents:}$在人工智能研究早期阶段，Symbolic AI 占据了主导地位，其特点是依赖于符号性的逻辑。早期的AI Agents主要关注两类问题： 1.transduction problem 2. respresentation/reasoning problem。这些Agents旨在模拟人类思考的方式，并且用于解决问题有明确的推理框架和可解释性。一个经典的例子就是专家系统，但是这类AgentsAgents的缺陷也很明显， 符号主义在处理不确定性和大量现实世界问题时仍有局限性，此外，符号推理赛算法复杂性高，想要平衡其时间效率和性能具有十足挑战性。\\n',\n",
       " '\\n',\n",
       " '\\u2003\\u2003$\\\\text{Reinforcement learning-based agents}$在强化学习（RL）的早期阶段，智能体主要依靠一些基础技术来进行学习，比如策略搜索和价值函数优化。其中，Q-learning和SARSA是比较著名的方法。 但是，随着深度学习技术的快速发展，我们将深度神经网络与强化学习相结合，形成了一种新的学习范式——深度强化学习（DRL）。这种结合让智能体能够从海量的高维数据中学习到复杂的策略，也带来了许多突破性的成果，比如AlphaGo和DQN。深度强化学习的强大之处在于，它让智能体能够在未知的环境中自主地进行学习，而不需要人类的干预或提供明确的指导。这种自主学习的能力，是AI领域的一大飞跃，也是未来智能体能够更好地适应复杂多变环境的关键。\\n',\n",
       " '\\n',\n",
       " '\\u2003\\u2003$\\\\text{LLM-based agents:}$随着大型语言模型（LLM）展现出令人瞩目的涌现能力，研究者们开始用其来打造新一代的人工智能代理。 他们将这些语言模型视为智能体的核心或“大脑”，并通过整合多模态感知和工具使用等策略，大大扩展了智能体的感知和行动能力。基于LLM的代理能够利用“思维链”（Chain of Thought，CoT）和问题分解（task decomposition）等技术，展现出与传统的符号智能体相媲美的推理和规划能力。 此外，它们还能够通过与环境的互动，从反馈中学习并执行新的动作。现如今，基于LLM的智能体已经被用于各种现实世界场景，比如用于软件开发与科学研究。 [[1](https://arxiv.org/abs/2309.07864v3)]\\n',\n",
       " '\\n',\n",
       " '## 智能体系统概述\\n',\n",
       " '\\n',\n",
       " '\\u2003\\u2003在由LLM驱动的智能体系统中，三个关键的不同组件使得LLM能充当智能体的大脑，从而驱动智能体完成不同的目标[[2]](https://lilianweng.github.io/posts/2023-06-23-agent/)。\\n',\n",
       " '\\n',\n",
       " '- **Reasoning & Planning.** 通过推理，智能体可以将任务细分成更简单、可执行程度更高的子任务。就像人类解决问题一样，基于一些证据和逻辑进行推理，因此，对于智能体来说推理能力 对于解决复杂任务至关重要。规划是人类应对复杂挑战时的核心策略。它帮助人类组织思维、确立目标，并制定实现这些目标的途径。对于智能体而言，规划能力同样关键，而这一能力取决于推理， 通过推理，智能体能够将复杂的任务分解为更易管理的子任务，并为每个子任务制定恰当的计划。随着任务的推进，智能体还能通过自省（反思）来调整其计划，确保它们与真实世界的动态环境保持一致，从而实现自适应性和任务的成功执行。 总的来说，推理和规划可以将复杂任务拆分成更易解决的子任务，同时通过反省之前的推理步骤，从错误中学习，并为未来的行动精炼策略，以提高最终结果的质量。\\n',\n",
       " '\\n',\n",
       " '- **Memory.**“记忆”存储智能体过去的观察、想法和行动的序列。正如人脑依赖记忆系统来回顾性地利用先前的经验来制定策略和决策一样，智能体需要特定的内存机制来确保它们熟练地处理一系列连续任务。 当人类面对复杂的问题时，记忆机制有助于人们有效地重新审视和应用先前的策略。此外，记忆机制使人类能够通过借鉴过去的经验来适应陌生的环境。此外，记忆可以分为短期记忆和长期记忆，对于基于LLM的智能体而言，In-context learning的内容可以视作LLM的短期记忆， 而长期记忆则是指给LLM提供一个向量知识库，智能体可以通过检索知识库获取其内部信息。\\n',\n",
       " '\\n',\n",
       " '- **Tool Use.** 当人们感知周围环境时，大脑会整合信息，进行推理和决策。人们通过神经系统控制身体，以适应或创造性行动，如聊天、避开障碍物或生火。 如果智能体拥有类似大脑的结构，具备知识、记忆、推理、规划和概括能力，以及多模式感知能力，那么它也被期望能够以各种方式对周围环境做出反应。而基于LLM的智能体的动作模块负责接收来自大脑模块的动作指令，并执行与环境互动的动作。 LLM收到指令再输出文本是其固有的能力，因此我们后继主要讨论其工具使用能力，也就是所谓的Tool Use。\\n',\n",
       " '\\n',\n",
       " '![agent_framwork](src\\\\AI-Agents\\\\agent_framwork.jpg)\\n',\n",
       " '\\n',\n",
       " '<center>*图 1: 基于LLM的智能体框架。*</center>\\n',\n",
       " '\\n',\n",
       " '# 1.Reasoning & Planning(推理和规划)\\n',\n",
       " '\\n',\n",
       " '## 1.1Reasoning\\n',\n",
       " '\\n',\n",
       " '\\u2003\\u2003Chain of Thought[[3]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).思维链技术逐渐成为大语言模型解决复杂类任务的标准方法，其通过在提示中加入几个具体的推理步骤提升大语言模型解决问题的性能，此外，有很多CoT的变种，如 Zero Shot CoT，其通过在提示中插入\"think step by step\"这样的一句话引导模型思考，推理出最终答案。\\n',\n",
       " '\\n',\n",
       " '\\u2003\\u2003Tree of Thoughts.[[4]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html)ToT通过在每个步骤探索多种推理可能性来扩展CoT。它首先将问题分解为多个思考步骤，并在每个步骤中生成多个想法，从而创建一个树状结构。 然后基于树状结构进行搜索寻求最优结果，搜索过程可以是BFS（广度优先搜索）或DFS（深度优先搜索），每个状态由分类器（通过提示）或多数投票评估。\\n',\n",
       " '\\n',\n",
       " '## 1.2Planning\\n',\n",
       " '\\n',\n",
       " '\\u2003\\u2003Least-to-Most[[5]](https://arxiv.org/abs/2205.10625)是提出问题后，先将其分割成若干小问题，然后一一解决这些小问题的一种策略。这种策略受到真实教育场景中用于指导儿童的策略的启发。 与CoT Prompting类似，这个策略首先将需要解决的问题分解成一系列子问题。子问题之间存在逻辑联系和渐进关系。在第二步，逐一解决这些子问题。 与CoT Prompting的最大差异在于，在解决下一个子问题时，会将前面子问题的解决方案作为提示输入。一个具体的字母连接的例子如下：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " 'Q: think, machine\\n',\n",
       " 'A: The last letter of \"think\" is \"k\". The last letter of \"machine\" is \"e\". \\n',\n",
       " 'Concatenating \"k\" and \"e\" gives \"ke\". So \"think, machine\" output \"ke\".\\n',\n",
       " '\\n',\n",
       " 'Q: think, machine, learning\\n',\n",
       " 'A: \"think, machine\" outputs \"ke\". The last letter of \"learning\" is \"g\". \\n',\n",
       " 'Concatenating \"ke\" and \"g\" gives \"keg\". So \"think, machine, learning\" is \"keg\".\\n',\n",
       " '\\n',\n",
       " 'Q: transformer, language\\n',\n",
       " 'A: The last letter of \"transformer\" is \"r\". The last letter of \"language\" is \"e\". \\n',\n",
       " 'Concatenating \"r\" and \"e\" gives \"re\". So \"transformer, language\" is \"re\".\\n',\n",
       " '\\n',\n",
       " 'Q: transformer, language, vision\\n',\n",
       " 'A:\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '![least2most](src\\\\AI-Agents\\\\least2most.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 2: least to most 输出。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\tReAct[[6]](https://arxiv.org/abs/2210.03629)通过将动作空间扩展为特定任务的离散动作和语言空间组合，成功将推理和行动集成在LLM中。前者能够使LLM生成自然语言进行推理，分解任务，后者则可以使LLM能够与环境交互（例如使用搜索API）。 ReAct prompt提示模板包含让LLM思考的明确步骤，从Langchain的源码中，我们可以找到如下：\\n',\n",
       " '\\n',\n",
       " '~~~markdown\\n',\n",
       " 'The way you use the tools is by specifying a json blob.\\n',\n",
       " 'Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n',\n",
       " 'The only values that should be in the \"action\" field are: {tool_names}\\n',\n",
       " 'The $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n',\n",
       " '```\\n',\n",
       " '{{{{\\n',\n",
       " '\"action\": $TOOL_NAME,\\n',\n",
       " '\"action_input\": $INPUT\\n',\n",
       " '}}}}\\n',\n",
       " '```\\n',\n",
       " 'ALWAYS use the following format:\\n',\n",
       " '\\n',\n",
       " 'Question: the input question you must answer\\n',\n",
       " 'Thought: you should always think about what to do\\n',\n",
       " 'Action:\\n',\n",
       " '```\\n',\n",
       " '$JSON_BLOB\\n',\n",
       " '```\\n',\n",
       " 'Observation: the result of the action\\n',\n",
       " '... (this Thought/Action/Observation can repeat N times)\\n',\n",
       " 'Thought: I now know the final answer\\n',\n",
       " 'Final Answer: the final answer to the original input question\\n',\n",
       " '~~~\\n',\n",
       " '\\n',\n",
       " '![ReAct](src\\\\AI-Agents\\\\ReAct.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 3: ReAct输出。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\tReflecxion[[7]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html)是一个为智能体提供动态记忆和自我反思能力从而提升智能体推理能力的框架。如下图所示，Reflexion框架中有三个角色，分别是Actor,Evaluator,Self-Reflection， 其中Actor模型基于观察到的环境和生成文本和动作，动作空间遵循ReAct中的设置，在ReActReAct中，特定任务的动作空间用语言扩展，以实现复杂的推理步骤。然后生成的这些文本和动作轨迹由Evaluator进行评估，比如用0,1来表示好坏，接着Self-Reflection会生成特定的文本反馈，提供更丰富、有效的反思并被存储到记忆中。最后，Actor会根据得到的记忆生成新的文本和动作直到完成任务。\\n',\n",
       " '\\n',\n",
       " '![Reflexion](src\\\\AI-Agents\\\\Reflexion.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 4: Reflexion 框架。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u2003\\t论文中提到，在序列决策任务**ALFWorld**上，为了实现自动化评估采用了两个技术：1.用LLM进行二分类 2.人工写的启发式规则检测错误。对于后者，简单来说就是 如果智能化执行相同操作并收到相同响应超过3个周期，或者当前环境中的操作次数超过30次(计划不高效)，就进行自我反思。\\n',\n",
       " '\\n',\n",
       " '\\u2003\\u2003$\\\\text{Chain of Hindsight(CoH)}$.[[8]](https://arxiv.org/abs/2302.02676)是一种引入了人类偏好的训练方法，该方法不仅使用正面反馈数据，还利用了负面反馈数据，此外在模型预测时引入了反馈条件 ，使模型可以根据反馈学习并生成更符合人类偏好的内容。但$\\\\text{CoT}$利用了序列形式的反馈，在训练时给模型提供更丰富的信息，具体如下：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '# How to explain a neural network to a 6-year-old kid? Bad:{a subpar answer} Good:{a good answer}.\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u2003\\t这样的数据拼接格式能组合不同种类的反馈，从而提升模型性能。在推理阶段，我们只需要在prompt中给模型指定好Good就能引导模型生成高质量的结果。 此外，在训练时并不是所有的token都纳入损失函数计算，feedback token即Good or Bad只用来提示模型接下来预测时生成质量好的内容还是差的内容，具体损失函数公式如下：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned} \\\\log p(\\\\mathbf{x})=\\\\log \\\\prod_{i=1}^{n} \\\\mathbb{1}_{O(x)}\\\\left(x_{i}\\\\right) p\\\\left(x_{i} \\\\mid\\\\left[x_{j}\\\\right]_{j=0}^{i-1}\\\\right) \\\\end{aligned} \\\\tag{1}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t\\t其中$\\\\mathbb{1}_{O(x)}$是指示函数，如果$x_i$属于$\\\\text{feedback token(Good or Bad)}$，则取值为$0$，反之为$1$。人类反馈数据，并不止单纯是先前所表述的只有$\\\\text{Good or Bad}$，它可以是更广义的形式，我们可以记作$D_h=\\\\{q,a_i,r_i,z_i\\\\}^{n}_{i=1}$，$q,a_i$分别是问题和答案，$r_i$是答案的评级高低，$z_i$则是人类提供的事后反馈。假设反馈的评级由低到高排的顺序是$r_{n} \\\\geq r_{n-1} \\\\geq \\\\cdots \\\\geq r_{1}$,那么在微调时数据拼接的形式如下：$d_h=(q,z_i,a_i,z_j,a_j,\\\\cdots,z_n,a_n),(i\\\\leq j\\\\leq n)$，训练时模型只会根据给定的前缀预测$a_n$，使得模型能够自我反映以基于反馈序列产生更好的输出。（笔者用小样本``900条数据``在7B级别的LLM上尝试微调时，发现如果反馈评级和回答$z_i,a_i,z_j,a_j,...,z_n,a_n$按照顺序排列，那么模型在推理时没法较好地适配不同反馈前缀，即生成的内容与反馈指定的预期的内容不符。比如$\\\\text{feedback token}$分别是“非提问式共情”和“提问式共情”，数据拼接格式为$\\\\text{“}q,\\\\underbrace{非提问式共情}_{\\\\text{feedback token}}a_1,\\\\underbrace{提问式共情}_{\\\\text{feedback token}}a_2\\\\text{”}$。模型在预测时即便给定了不同的$\\\\text{feedback token}$，其对于提问式共情的感知能力仍然不足，一直会进行非提问式共情。）因此笔者猜测如果采用小批量数据在规模较小的模型上训练会导致位置偏差(Position Bias)，如果能把不同评级的训练数据打乱或许能增强模型的泛化能力，促使模型理解不同$\\\\text{feedback token}$代表的含义而不是仅仅记住其位置特性。\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '# 我最近心情很糟糕 非提问式共情:{听起来你真的很难受呢} 提问式共情:{我真是替你感到难过，请问你现在赶紧怎样呢？想和我聊一聊吗？}.\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在训练中，由于模型在预测时条件化了另一个模型输出及其反馈，因此可能会简单地“复制”已提供的示例，而不是真正理解任务。 为了防止模型只“复制”示例而不学习任务，作者在训练时随机掩盖了$0\\\\%$到$5\\\\%$的历史$\\\\text{token}$。这意味着模型无法直接看到这些$\\\\text{token}$，从而需要真正地理解上文，而不是简单地复制，这种随机掩盖增加了模型的泛化能力。\\n',\n",
       " '\\n',\n",
       " '![CoT](src\\\\AI-Agents\\\\CoT.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 5: Chain of Hindsight论文实验。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t该文对RLHF和$\\\\text{CoH}$方法进行了实验对比，图中的蓝色柱是标准指令上的得分，从整体上看，RLHF在标准指令上最终的效果还是要比$\\\\text{CoH}$要好的，但是当条件词改变，让模型输出更好的质量的内容时，$\\\\text{CoH}$生成的结果质量要比RLHF要好，而且对于不同条件词的变化更为敏感，说明模型较好地理解了人类偏好。改论文实现方法也比较简单，也有所缺陷，比如对一个回答要准备多条（对应不同的偏好），假设需要训练一个十轮对话，每一轮对话的偏好为$K$个，那么需要标注$10\\\\times K$\\u200b的回答语料，极大程度上依赖于人力。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在推理时，不同于原始对话历史拼接方式，我们需要在末尾额外加上$\\\\text{feedback token}$。假设$\\\\text{feedback token}$分别是$\\\\text{good answer:}$和$\\\\text{bad answer:}$，则具体如下：\\n',\n",
       " '\\n',\n",
       " '```shell\\n',\n",
       " '#假设正常的多轮对话数据拼接方式为\\n',\n",
       " 'user:我今天心情不好。</s>assistant:请问发生什么了宝贝?\\n',\n",
       " '#采用CoH以后的多轮对话数据拼接方式会变为\\n',\n",
       " 'user:我今天心情不好。</s>assistant:<good answer>:请问发生什么了宝贝?<bad answer>:我理解你的感受。\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '![image-20241029105729307](E:\\\\Study\\\\知识构建\\\\NLP系列\\\\assets\\\\image-20241029105729307.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 6: Chain of Hindsight模式下的token拼接。*</center>\\n',\n",
       " '\\n',\n",
       " '缺点：部署模型推理服务时不能直接使用Tokenizer提供的```apply_chat_template```的方法，需要工程师自定义实现多轮对话的拼接方式。\\n',\n",
       " '\\n',\n",
       " '原论文提供了jax的实现，代码在[Github](https://github.com/haoliuhl/chain-of-hindsight/blob/main/coh/coh_train_gptj.py)，笔者根据自己的理解基于Pytorch实现的代码链接[如下](https://github.com/YanZiBuGuiCHunShiWan/LLM_training)。\\n',\n",
       " '\\n',\n",
       " '# 2.Memory(记忆)\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在神经科学领域，人类的记忆被分为几种不同的类型，这些类型在信息处理和存储方面各有特点，具体如下：\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t1.**感觉记忆（Sensory Memory）**这是记忆系统的第一阶段，非常短暂，它保存了来自感官的信息，如视觉和听觉，通常只持续几秒钟。例如，当你看到一串数字时，你能在短时间内记住它们，这就像计算机中的缓存，用于临时存储。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t2.**短期记忆（Short-term Memory）**短期记忆是你在短时间内能够主动保持和操纵的信息。它有有限的容量，通常可以持续大约20-30秒，通过重复或其他策略可以转移到长期记忆。这就像计算机的RAM，可以快速读写，但断电后信息会丢失。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t3.**长期记忆（Long-term Memory）**长期记忆是信息可以存储很长时间的地方，从几个小时到一生。它分为两种主要的子类型：**(a)**显性记忆（Explicit Memory）这是有意识的记忆，可以进一步分为事实和信息（语义记忆）以及个人经历（情景记忆）。就像在硬盘上存储的文件，你可以有意识地检索这些信息。**(b)**隐性记忆（Implicit Memory）这是无意识的记忆，包括技能、习惯和条件反射（骑单车、敲键盘）。这些记忆不像显性记忆那样容易被意识到，但它们影响你的行为和反应。这就像计算机的BIOS或操作系统设置，你通常不直接与它们交互，但它们影响计算机如何运行。我们可以将智能体和记忆按照如下映射进行理解：\\n',\n",
       " '\\n',\n",
       " '- 感觉记忆就像原始输入对应的语义嵌入，包括文本、图像和其他模态。\\n',\n",
       " '\\n',\n",
       " '- 短期记忆就像上下文学习($\\\\text{In-context learning}$)，是有限的，受到$\\\\text{LLM}$上下文窗口限制。\\n',\n",
       " '\\n',\n",
       " '- 长期记忆就像外部向量库，智能体可以通过快速检索从向量库中抽取相关的信息进行阅读理解。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t随着 AI 的不断演化及其应用领域的扩展，为其赋予长期记忆功能变得愈发重要，长期记忆在多方面提升了 AI 系统的能力：以LLM+RAG范式的问答系统而言，长期记忆能有效地缓解大语言模型的幻觉现象，提升在垂直领域的专业表现力。其次通过存储关键信息结合时序检索可以在当前和过去场景建立联系，或是了解用户过往信息如兴趣爱好等从而增强智能体的适应能力。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在AI越来越强大的今天，人们开始有了自己的\"AI助手\"，甚至开始体验AI虚拟男女友，”AI陪伴“逐渐成为一个有潜力的市场。一个好的AI陪伴产品，应当像一个朋友，做到”懂我“，陪伴不仅仅是对用户表明需求的响应，而是一种深入到个体心理和情感层面的理解，比如了解用户喜好、兴趣和习惯，甚至与用户价值观和信念共鸣[[9]](https://mp.weixin.qq.com/s/zcSMerSKX30P2Hrwhoh0TA)。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t从技术层面讲，”懂我“的基础是长期记忆，智能助手只有记得之前发生的事情，才能精准把握用户偏好，了解用户成长中的重要时刻，行为习惯。而智能助手的的记忆力一直是各个团队的研究重点，至今并未看到成型的解决方案。为此，笔者搜索并总结了部分关于AI智能体长期记忆相关方面的前沿研究。\\n',\n",
       " '\\n',\n",
       " '## 2.1 MemoryBank\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**MemoryBank** [[10]](https://ojs.aaai.org/index.php/AAAI/article/view/29946)聚焦于如何为大语言模型（LLMs）设计高效的**长期记忆机制**。该研究旨在解决当前生成式 AI 在长期对话中记忆能力有限的问题。通过设计一套基于外部记忆存储的系统，MemoryBank 可以让LLM回顾历史交互，逐渐增强对上下文的理解并适应和用户过往的交互对话，逐渐增强在长期交互场景下的对话表现。其根据艾宾浩斯遗忘曲线理论设计了一个和人类认知过程相似的动态记忆机制，可以让LLM随着时间流逝记忆、选择性遗忘或强化记忆。**MemoryBank**是一个围绕三个核心支柱构成的统一机制：（1）记忆存储（2）记忆检索（3）记忆更新。\\n',\n",
       " '\\n',\n",
       " '![image-20241118091018866](E:\\\\Study\\\\知识构建\\\\NLP系列\\\\assets\\\\image-20241118091018866.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 7: MemoryBank框架。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t记忆存储是一个包含三种不同层次信息的数据库：（1）附带了时间戳信息的过往对话。（2）基于对话提炼出的每日事件总结。（3）动态用户个性了解，通过长期的互动不断地评估创造日常的关于用户的个性洞察的见解，进一步整合见解以形成较为全面的用户画像，从而促使LLM根据用户信息学习、适应和调整其回复内容，增强用户体验。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t其记忆检索机制基于$\\\\text{FAISS}$向量数据库，每一轮的对话和事件总结被视作一个记忆块$m$，通过一个预训练的双塔编码器将$m$编码成语义向量$h_m$，最终构成一个记忆集合$M=\\\\{h_m^{0},h_m^{1},h_m^{2},...,h_m^{|M|}\\\\}$。用户对话时其输入$c$会先被编码成向量$h_c$，然后从$M$中检索到最相似的$h_{m}^{j},j=0,...,|M|$作为相关记忆。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t对于那些期望更具人类化记忆行为的应用场景，则需要进行记忆更新对过往地很少被回溯的记忆片段进行遗忘。如AI陪伴等，可以使AI伴侣显得更加自然。**MemoryBank**的记忆更新机制基于艾宾浩斯遗忘曲线并遵循如下几个原则：\\n',\n",
       " '\\n',\n",
       " '- **遗忘率(Rate of Forgetting) ** 记忆保留能力会随着时间的推移而减少。艾宾浩斯遗忘曲线中量化了这一点，表明在学习后，除非有意识地回顾信息，否则信息会迅速丢失。\\n',\n",
       " '- **时间和记忆衰减(Time and Memory Decay)** 艾宾浩斯遗忘曲线在一开始十分陡峭，这表明在学习后的最初几个小时或几天内，大量的学习信息被遗忘了。在这个初始阶段之后，记忆丧失的速度就会减慢。\\n',\n",
       " '- **间距效应(Spacing Effect)**  艾宾豪斯发现，重新学习信息比第一次学习信息更容易。定期回顾和重复学习到的知识可以重置遗忘曲线，使其不那么陡峭，从而增强对知识的长久印象。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t艾宾浩斯遗忘曲线的公式是$\\\\begin{aligned}R=e^{-\\\\frac{t}{S}} \\\\end{aligned}$，其中$S$代表记忆强度，其依赖于如学习深度或者重复次数的关键因子，$t$代表学习知识后经过的时间，当记忆在对话中被回顾以后，将会增加$S$然后将$t$\\u200b重新设置为0。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t论文并为过多详细阐述技术实现细节，整体篇幅不长，易于理解。\\n',\n",
       " '\\n',\n",
       " '## 2.2 Generative Agents\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t斯坦福大学的研究-**Generative Agents: Interactive Simulacra of Human Behavior**[[11]](https://dl.acm.org/doi/abs/10.1145/3586183.3606763)，（即 AI 小镇）是一项极具开创性的尝试，展示了生成式 AI 模型在模拟人类行为和社会互动中的潜力。这篇论文中，研究人员构建了一个虚拟的模拟环境——“AI 小镇”，并赋予其中的虚拟角色（Generative Agents）自主性、记忆力以及持续的行为和社会交互能力。这些角色可以模仿人类的日常活动，包括规划自己的日程、参与对话、形成关系，并在交互中基于长期记忆调整行为。具体地，研究人员设计了一个可以存储、合成和应用相关记忆生通过LLM生成可靠行为的智能体框架，该框架有三个核心：\\n',\n",
       " '\\n',\n",
       " '- **Memory stream（记忆流）** 记忆流是一个长期记忆模块，以自然语言记录智能体的全面经历列表。记忆检索模型结合相关性、时效性和重要性，从中提取所需记录，以支持智能体的实时行为决策\\n',\n",
       " '- **Reflection (反思) ** 将记忆综合为更高层次的推论，使智能体能够随着时间推移得出关于自身和他人的结论，从而更好地引导其行为。\\n',\n",
       " '- **Planning and ReActing（规划和反应）** 将这些结论及当前环境转化为高层次的行动计划，然后递归地细化为具体的行为与反应。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t这些不同层次的记忆，即反思和计划会被反馈回记忆流中，对智能体后继的行为与决策产生影响。\\n',\n",
       " '\\n',\n",
       " '### 2.2.1 Memory Retrieval \\n',\n",
       " '\\n',\n",
       " '\\u200b\\t原文中说到检索由三个要素构成：\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Recency (最近性)** 最近被访问的记忆对象分配更高的分数，使得智能体能够更容易关注到刚刚发生或今天早些时候的事件。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Importance（重要性）**则通过为智能体认为重要的记忆对象分配更高分数，来区分日常琐事与核心记忆。例如，一件平凡的事情，比如在自己房间吃早餐，会得到较低的重要性评分，而一件重大事件，比如与伴侣分手，则会得到较高评分。重要性评分有多种可能的实现方式，原文中的实现方式是通过提示工程让大语言模型进行评分，prompt如下：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '\"\"\"On the scale of 1 to 10, where 1 is purely mundane\\n',\n",
       " '(e.g., brushing teeth, making bed) and 10 is\\n',\n",
       " 'extremely poignant (e.g., a break up, college\\n',\n",
       " 'acceptance), rate the likely poignancy of the\\n',\n",
       " 'following piece of memory.\\n',\n",
       " 'Memory: buying groceries at The Willows Market\\n',\n",
       " 'and Pharmacy\\n',\n",
       " 'Rating: <fill in>\"\"\"\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Recency（相关性）**为与当前情境相关的记忆对象分配更高的分数。例如，如果查询是某个学生正与同学讨论如何准备化学考试，那么与早餐相关的记忆对象的相关性应较低，而与老师和课业相关的记忆对象的相关性应较高。原文采用了语义相似度作为相关性的衡量，即通过预训练模型编码器将每条记忆文本编码为语义向量，通过计算记忆向量和当前查询向量的语义相似度来确定相关性。\\n',\n",
       " '\\n',\n",
       " '![image-20241118105509468](E:\\\\Study\\\\知识构建\\\\NLP系列\\\\assets\\\\image-20241118105509468.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 8: GenAI记忆流。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t为了计算最终的检索分数将**新近性**、**相关性**和**重要性**的分数使用最小-最大缩放法$(\\\\operatorname{min-max scaling})$归一化到$ [0, 1]$\\u200b\\u200b 范围。检索函数将所有记忆的分数计算为三者加权组合的形式：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}score=\\\\alpha_{recency}⋅recency+\\\\alpha_{importance}⋅importance+\\\\alpha_{relevance}⋅relevance \\\\end{aligned} \\\\tag{2}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t与MemoryBank不同，Gen AI中并未提及具体的记忆更新机制，即智能体所有发生过的行为都会被记录，不会选择性地遗忘或者强化，随着行为的增加，检索的记忆量也随之上升，如果检索数量没有限制为$\\\\text{Top1}$则之前发生的事情未被遗忘或更新可能导致检索出相关冲突片段影响智能体的决策。比如用户在8月份某一天的白天八点钟提及“我今晚想去看一场电影《变形金刚》。”，晚上七点的时候说“我改变主意了，我想看《功夫熊猫》”。在9月份，如果用户问智能助手“我上月看了什么电影？”，那么检索出来的很可能会遗漏“我改变主意了，我先看《功夫熊猫》”这句相关信息，导致智能助手未能正确判断用户最后看了什么电影。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t引入时间维度信息检索可以更有效地帮助智能体检索到近期发生的事件，**TempRALM**[[12]](https://arxiv.org/abs/2401.13222)模型虽未直接研究如何更好地实现多智能体的长期记忆机制，但其提出的融入时间戳信息的检索机制值得借鉴。具体地，TempRALM在检索的召回阶段将语义相似度和时间相关性纳入考量：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned} TempRet_t(q,d,q_t,d_t) = s(q,d)+\\\\tau(q_t,d_t)\\\\end{aligned} \\\\tag{3}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t$TempRet_t(q,d,q_t,d_t)$是赋分函数，而$s(q,d)$就是查询与文档的语义相似度，$\\\\tau(q_t,d_t)$则是融入时间信息的函数，其主要遵循如下原则：(1)如果时间差过大，则得分应该较低。（2）需要和语义相似度数值处于一个量级（不同量级则会导致其中一方不起作用，比如一个是$10^{2}$级别，另一个是$10^{-1}$级别。）。最终的$\\\\tau(q_t,d_t)$形式如下：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}{\\\\hat \\\\tau(q_t, d_t)}&=\\\\frac{\\\\alpha_{scale}}{q_t-d_t}\\\\\\\\\\n',\n",
       " '\\\\tau(q_t, d_t)&=\\\\frac{\\\\hat \\\\tau(q_t, d_t)-\\\\mu_{\\\\tau}}{\\\\sigma_{\\\\tau}} \\\\times \\\\sigma_{s}+\\\\mu_{s}\\\\end{aligned}\\\\tag{4}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t其中，$\\\\alpha$是缩放因子，$\\\\mu_t$是所有时间分数的均分，$\\\\sigma_\\\\tau$是时间分数的标准差，$\\\\sigma_{s}$是语义相关性的标准差，$\\\\mu_s$是语义相关性的均值。\\n',\n",
       " '\\n',\n",
       " '### 2.2.2 Reflection\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t反思是由智能体生成的更高层次、更抽象的思想。由于反思也是一种记忆，它在检索时会与其他观察记录一同被包含在记忆流。当智能体最近感知到的事件的重要性分数之和超过某一阈值（原文设定为 150）时，就会触发反思生成。在实验中，智能体每天大约会进行两到三次反思。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t第一步是是确定反思的主题，智能体需要根据最近的经历判断可以提出的问题。从智能体的记忆流中提取最近的 100 条记录，例如：“Klaus Mueller正在阅读关于绅士化的书籍”，“Klaus Mueller正在与图书管理员讨论他的研究项目”，“图书馆的桌子目前无人占用”，然后向语言模型发送以下提示：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '\"\"\"Given only the information above, what are 3 most salient highlevel questions we can answer about the subjects in the statements?\"\"\"\\n',\n",
       " '#仅根据上述信息，可以回答的三个最重要的高层次问题是什么？\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t模型的回答会生成候选问题，例如：“Klaus Mueller热衷于哪些话题？”以及“Klaus Mueller和Maria Lopez之间是什么关系？\"，然后使用这些生成的问题作为查询，并为每个问题从记忆流中收集相关记忆（包括其他反思）。随后，通过提示工程让大语言模型提取见解，并引用支持这些见解的具体记录。完整的提示如下：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '\"\"\"Statements about Klaus Mueller\\n',\n",
       " '1. Klaus Mueller is writing a research paper\\n',\n",
       " '2. Klaus Mueller enjoys reading a book\\n',\n",
       " 'on gentrification\\n',\n",
       " '3. Klaus Mueller is conversing with Ayesha Khan\\n',\n",
       " 'about exercising [...]\\n',\n",
       " 'What 5 high-level insights can you infer from\\n',\n",
       " 'the above statements? (example format: insight\\n',\n",
       " '(because of 1, 5, 3))\"\"\"\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t这一过程生成了诸如以下的反思陈述：$\\\\text{“克劳斯·穆勒致力于研究绅士化（基于1, 2, 8, 15）”。}$\\u200b，随后语言模型解析（从回复中抽取有意义的文本内容）并将该陈述作为反思存储在记忆流中，同时包括关于记忆对象的引用（笔者的理解是那些数字序号）。反思机制明确允许智能体不仅反思其观察到的内容，还可以反思其他反思。例如，上述关于克劳斯·穆勒的第二条陈述就是他之前进行的一次反思，而不是环境中的观察。因此，智能体可以生成反思树：树的叶节点代表基础观察记录（智能体的一系列行为），非叶节点则代表随着层级上升逐渐变得更抽象、更高层次的思想（比如反思），如下图所示：![image-20241118114930476](E:\\\\Study\\\\知识构建\\\\NLP系列\\\\assets\\\\image-20241118114930476.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 9: GenAI不同层次记忆。*</center>\\n',\n",
       " '\\n',\n",
       " '### 2.2.3 Planning and ReActing \\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Planning.**尽管大型语言模型能够根据情境信息生成合理的行为，但为了确保行为序列在更长的时间跨度内保持可靠，智能体需要进行规划。如果仅通过提示语言模型提供克劳斯的背景信息、时间描述，并询问他此刻应该做什么，他可能会在中午12点吃午餐，但又会在12:30和1点再次吃午餐，即使他已经吃过两次了。如果只考虑当前时刻的行为可靠性则会牺牲长时的行为可靠性。为了解决这个问题，规划是必不可少的。克劳斯的下午计划这样将会更合理：他在12点到霍布斯咖啡馆边吃午餐边阅读，1点在学校图书馆进行研究论文的撰写，3点去公园散步放松。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t规划描述了智能体未来的一系列行动，并帮助智能体在长时间内保持行为一致性。一个规划包括地点、开始时间和持续时间。例如，克劳斯·穆勒致力于他的研究并且马上要面临截止日期，他可能会选择整天在自己的书桌前撰写研究论文。一条规划可能是：“2023年2月12日上午9点起，奥克山学院宿舍克劳斯·穆勒的房间内书桌处，进行180分钟的阅读和研究论文笔记。”和反思一样，规划也会被存储在记忆流中，并在检索过程中与观察和反思一起被考虑。这使得智能体能够综合这些信息做出行为决策，并在必要时实时调整计划。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t对于智能体来说，不能只大致考虑要做什么，而要考虑到更合理与细致的行为信息，比如一个画家智能体不可能计划在要点柜台前一直坐四个小时不动。更理想的情况是：在四小时的工作室时间里，智能体规划好时间进行材料收集、调制颜色、休息和清理。通过自顶向下的方式，递归生成智能体更细致的行动，一个prompt示例如下：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '\"\"\"Name: Eddy Lin (age: 19)\\n',\n",
       " 'Innate traits: friendly, outgoing, hospitable\\n',\n",
       " 'Eddy Lin is a student at Oak Hill College studying\\n',\n",
       " 'music theory and composition. He loves to explore\\n',\n",
       " 'different musical styles and is always looking for\\n',\n",
       " 'ways to expand his knowledge. Eddy Lin is working\\n',\n",
       " 'on a composition project for his college class. He\\n',\n",
       " 'is taking classes to learn more about music theory.\\n',\n",
       " 'Eddy Lin is excited about the new composition he\\n',\n",
       " 'is working on but he wants to dedicate more hours\\n',\n",
       " 'in the day to work on it in the coming days\\n',\n",
       " 'On Tuesday February 12, Eddy 1) woke up and\\n',\n",
       " 'completed the morning routine at 7:00 am, [. . . ]\\n',\n",
       " '6) got ready to sleep around 10 pm.\\n',\n",
       " 'Today is Wednesday February 13. Here is Eddy’s\\n',\n",
       " 'plan today in broad strokes: 1)\"\"\"\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t这将会生成一个粗略计划，通常为5到8个部分。例如：“1）早上8点起床并完成早间例行；2）10点前往奥克山学院上课；[...]；5）下午1点到5点进行新作曲项目；6）下午5:30吃晚餐；7）晚上11点前完成作业并入睡。”。智能体将此计划存储在记忆流中，并递归地将其细化为更具体的行动。首先，将其分解为每小时的行动块，例如：“下午1点至5点进行新作曲项目”被细化为：“下午1点开始头脑风暴，为作曲项目提出一些新想法；[...]；下午4点短暂休息以恢复创意活力，然后审阅并润色作曲。”。然后再次递归分解为5到15分钟的时间块，例如：“下午4点：吃点水果、燕麦棒或坚果等轻便零食；下午4:05：在工作空间附近散步；[...]；下午4:50：花几分钟清理工作空间。”，这个过程可以根据想要的细粒度进行调整。\\n',\n",
       " '\\n',\n",
       " \"\\u200b\\t**ReActing.** 智能体在一个行动循环中运行。在每个时间步，智能体会感知周围的世界，这些感知到的观察会被存储到它的记忆流中。我们通过提示语言模型基于这些观察来决定智能体是**继续执行其现有计划，还是做出反应**。例如，当智能体站在画架前作画时，可能会触发对画架的观察，但这通常不会触发反应。然而，如果埃迪的父亲约翰记录到他看到埃迪正在花园散步，结果就会不同。以下是提示的内容，其中 [Agent's Summary Description] 是一个动态生成的概述智能体目标和性格特征的段落：\\n\",\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '[Agent’s Summary Description]\\n',\n",
       " 'It is February 13, 2023, 4:56 pm.\\n',\n",
       " 'John Lin’s status: John is back home early from\\n',\n",
       " 'work.\\n',\n",
       " 'Observation: John saw Eddy taking a short walk\\n',\n",
       " 'around his workplace.\\n',\n",
       " 'Summary of relevant context from John’s memory:\\n',\n",
       " 'Eddy Lin is John’s Lin’s son. Eddy Lin has been\\n',\n",
       " 'working on a music composition for his class. Eddy\\n',\n",
       " 'Lin likes to walk around the garden when he is\\n',\n",
       " 'thinking about or listening to music.\\n',\n",
       " 'Should John react to the observation, and if so,\\n',\n",
       " 'what would be an appropriate reaction?\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t上下文总结是通过两个查询提示生成的：**“What is [observer]’s relationship with the [observed entity]?”**、**“[Observed entity] is [action status]”**，模型的输出建议约翰可以考虑询问埃迪关于他的作曲项目的情况。随后，我们会从发生反应的时间点开始重新生成智能体的现有计划。最后，如果该行动表明智能体之间会发生互动，就生成他们的对话内容。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t对话时，通过将智能体的记忆与对方的相关记忆作为条件，生成他们的对话内容。例如，当约翰与埃迪开始对话时，会先根据约翰关于埃迪的总结性记忆，以及他决定询问埃迪作曲项目时的预期反应，生成约翰的第一句话：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '[Agent’s Summary Description]\\n',\n",
       " 'It is February 13, 2023, 4:56 pm.\\n',\n",
       " 'John Lin’s status: John is back home early from\\n',\n",
       " 'work.\\n',\n",
       " 'Observation: John saw Eddy taking a short walk\\n',\n",
       " 'around his workplace.\\n',\n",
       " 'Summary of relevant context from John’s memory:\\n',\n",
       " 'Eddy Lin is John’s Lin’s son. Eddy Lin has been\\n',\n",
       " 'working on a music composition for his class. Eddy\\n',\n",
       " 'Lin likes to walk around the garden when he is\\n',\n",
       " 'thinking about or listening to music.\\n',\n",
       " 'John is asking Eddy about his music composition\\n',\n",
       " 'project. What would he say to Eddy?\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t对话结果是:“Hey Eddy, how’s the music composition project for your class coming along?”，从埃迪的视角来看，约翰开始这段对话被视为一个他可能想要做出反应的事件。因此，与约翰类似，埃迪会检索并总结他关于与约翰关系的记忆，以及可能与约翰刚刚的对话内容相关的记忆。如果他决定回应，我们会基于埃迪的记忆总结和当前的对话历史生成他的回答：\\n',\n",
       " '\\n',\n",
       " '```markdown\\n',\n",
       " '[Agent’s Summary Description]\\n',\n",
       " 'It is February 13, 2023, 4:56 pm.\\n',\n",
       " 'Eddy Lin’s status: Eddy is taking a short walk\\n',\n",
       " 'around his workplace.\\n',\n",
       " 'Observation: John is initiating a conversation\\n',\n",
       " 'with Eddy.\\n',\n",
       " 'Summary of relevant context from Eddy’s memory:\\n',\n",
       " 'John Lin is Eddy Lin’s father. John Lin is caring\\n',\n",
       " 'and is interested to learn more about Eddy Lin’s\\n',\n",
       " 'school work. John Lin knows that Eddy Lin is\\n',\n",
       " 'working on a music composition.\\n',\n",
       " 'Here is the dialogue history:\\n',\n",
       " 'John: Hey Eddy, how’s the music composition project\\n',\n",
       " 'for your class coming along?\\n',\n",
       " 'How would Eddy respond to John?\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t然后得到Eddy的回应:“Hey Dad, it’s going well. I’ve been taking walks around the garden to clear my head and get some inspiration.”，接下来的对话将通过相同的机制生成，直到其中一个智能体决定结束对话为止。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Memory stream,Reflection,Planning and ReActing** 三种层次的记忆联合让智能体根据过往的经历学会合理地思考，规划接下来细致的活动以及根据当前环境选择是继续规划还是做出反应，让智能体的思考与行为更像人类。文中还分析了智能体之间的信息传播现象，即测量了两条特定信息在两天内的传播情况，并观察到了智能体社区在模拟期间建立了新的关系，社交网络密度大幅增长，受篇幅和重点限制，笔者就不详细阐述了。\\n',\n",
       " '\\n',\n",
       " '## 2.3 模拟人类记忆过程\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t人类记忆系统为设计 AI 长期记忆提供了宝贵的启示。近年来的研究表明，部分 AI 系统在不同程度上模仿并融入了与人类长期记忆结构相似的机制，认知架构研究也采用了人类长期记忆的关键组成部分：情景记忆、语义记忆和程序性记忆[[13]](https://arxiv.org/abs/2411.00489)。Chessa[[14]](https://pubsonline.informs.org/doi/abs/10.1287/mksc.1060.0212)等人基于Zielske提出的回忆概率函数，提出了一种模型，该模型假设记忆巩固速率$r(t)$表示人类记忆在某时间点被回忆的概率$p(t)$，公式如下：\\n',\n",
       " '$$\\n',\n",
       " 'p(t)=1-\\\\sum_{n=1}^{b-1} \\\\frac{(r(t))^{n}}{n!} \\\\exp (-r(t)) \\\\tag{5}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t这个模型基于这样一个假设，即每个神经元是独立且随机地发放信号的，它源自于使用时间变化强度函数𝑟(𝑡)的[非齐次泊松过程]()的性质。该模型还考虑了回忆所需的刺激阈值𝑏。指数函数𝑟(𝑡)代表了人类海马体中记忆强度的调整过程：\\n',\n",
       " '$$\\n',\n",
       " 'r(t)=\\\\mu e^{-\\\\alpha t} \\\\tag{6}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t其中，$\\\\mu$是记忆强度，$\\\\alpha$是衰减率，$t$代表经过的时间，在用向量数据库召回时，只考虑一个事件，因此将$b$设置为1，则召回概率$p(t)$表达式如下：\\n',\n",
       " '$$\\n',\n",
       " 'p(t)=1-\\\\exp(-\\\\mu e^{-\\\\alpha t}) \\\\tag{7}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t回顾概率随着时间$t$呈指数衰减，然而多次被回忆的记忆和未被回忆的记忆在巩固程度上有差异，因此遗忘速率应作相应调整来反应这一影响。在前一小节所提到的AI小镇实现了三种层次的记忆，记忆检索机制考虑到了记忆的时效性(recency)，重要程度(importance)和相关性(relevance)。**Hou**[[15]](https://dl.acm.org/doi/abs/10.1145/3613905.3650839)等人认为考虑时间流逝、相关性和回忆频率来计算记忆巩固程度能让智能体能够唤起最合适的回忆，促进有效对话。前者侧重于独立为每一个记忆进行评分，通过综合得分选择最适合当前情境下的行动，后者则通过随着时间的推移调整记忆巩固程度，确保记忆的一致性。[Hou]()等人将事件相关性$r$和经过的时间$t$作为变量，带入公式(7)则得到如下的记忆回顾概率公式：\\n',\n",
       " '\\n',\n",
       " '$$\\n',\n",
       " 'p(t)=1-\\\\exp(-r e^{-\\\\alpha t}) \\\\tag{8}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t事件相关性可以通过语义向量的余弦相似度衡量，而衰减常数$\\\\alpha$则需要根据事件被回忆的次数进行调整，考虑到$n$次被回忆的事件对应的$\\\\alpha$公式如下：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}a&=\\\\frac{1}{g_n},\\\\text{ }g_0=1 \\\\\\\\\\n',\n",
       " 'g_n&=g_{n-1}+S(t),\\\\text{ } S(t)=\\\\frac{1-e^{-t}}{1+e^{-t}}\\\\end{aligned} \\\\tag{9}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t修改后的 $\\\\operatorname {sigmoid} $函数 $S(t)$ 表示记忆在每次回忆后的巩固过程，并在$t>0$时单调增加。然而，每次回忆导致的$\\\\alpha$的减少是有上限的，以反映长期记忆的巩固。随着回忆次数$n$的增加，$\\\\alpha$的减少速率逐渐降低，这模拟了人类自然记忆的过程，即频繁的回忆会加强记忆的巩固，最终的公式如下：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}\\n',\n",
       " 'p_{n}(t) & =\\\\frac{1-\\\\exp \\\\left(-r e^{-t / g_{n}}\\\\right)}{1-e^{-1}} \\\\\\\\\\n',\n",
       " 'g_{n} & =g_{n-1}+\\\\frac{1-e^{-t}}{1+e^{-t}}\\n',\n",
       " '\\\\end{aligned} \\\\tag{10}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t最终通过设定一个召回阈值$k$确定当前的记忆是否要回忆。公式(8)的函数绘制如下（另$r$=1,$\\\\alpha$和$t$是变量）：\\n',\n",
       " '\\n',\n",
       " '![image-20241118170422418](assets\\\\image-20241118170422418.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 10: p(t)函数绘制图像。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t图中可以看到，假设固定住时间$t$，则随着$a$的增大，回顾的概率$p(t)$会减少，反之则增大，而$a$是一个与事件回忆频率变化的数，事件回忆次数越高，则$a$越小，同时也能表明这件事情比较重要，而被回顾的概率也就越高。论文中的实验部分展示该模型和Gen AI(斯坦福的AI小镇)的一些差异之处：\\n',\n",
       " '\\n',\n",
       " '![image-20241118175412825](assets\\\\image-20241118175412825.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 11: 实验案例。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t当用户说：”我下周四要和我朋友去音乐会“。本文的模型(model1)生成的内容明显依赖于用户的历史行为（例如，事件 A：周四在大学工作的记录)，而未能适应用户提供的新情境（因为模型回复时提及过往的情境)。这表明该模型在面对用户行为的偏离时存在局限性，它更倾向于优先考虑长期模式和事件的重要性，而非当前的情境。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t相比之下，Generative Agents (model2)模型使用了一个更简单的评分系统，该系统基于事件的**最近性**、**重要性**和**相关性**，因此选择了事件**D**（周四在餐馆工作）作为最可能的活动。这一选择源于模型对近期活动和事件相关性的重视，可以从事件**D**关联的较高相关性评分以及更短的时间间隔看出。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t两种模型生成的不同内容突显了不同设计理念的差异：[Hou]()提出的模型注重长期记忆的整合，而 Generative Agents 模型更注重近期和相关性较高的事件。即Hou的方法的主要局限性在于依赖于用户长期的行为模式来计算记忆巩固的概率$p(t)$\\u200b\\u200b。如果用户行为忽然变化（如开始新工作或学校，生活方式改变），方法的适应性可能有限。虽然该方法考虑了时间因素、回忆频率、相关性因素，但还可以考虑引入其他的因素进一步优化，比如心理场景下考虑记忆的情感重要程度等，可能会进一步提升记忆巩固的效果。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Hippo RAG**[[16]](https://arxiv.org/abs/2405.14831) 借鉴**海马体记忆索引理论(hippocampal memory indexing theory)**模拟人类大脑中海马体的功能，提供动态、渐进式的知识更新机制。海马体索引理论是一个成熟的理论，为人类长期记忆中涉及的组成部分和回路提供了功能性的描述。在该理论中Teyler和Discenna提出人类长期记忆由新皮质(Neocortex)、海马旁回区域(Parahippocampal Regions)、海马体(Hippocampus)三个组件组成，它们协同工作以完成**模式分离(Pattern separation)**和**模式补全(Pattern completion)**的功能。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t该理论中，模式分离主要在记忆编码过程中完成。记忆编码首先由新皮质接收和处理感知刺激，将其转化为更易操作的（可能是更高层次的）特征，这些特征随后通过**海马旁回区域（PHR）**传递到海马体进行索引。当信号到达海马体时，显著的信号会被包含在海马索引中，并彼此关联。在记忆编码过程完成后，模式补全驱动记忆检索过程。当海马体通过 PHR 管道接收到部分感知信号时，海马体利用其**情境依赖的记忆系统**在海马索引中识别完整且相关的记忆，并通过 PHR 将其路由回新皮质进行模拟。因此，这一复杂过程仅改变海马索引来整合新信息，而无需更新新皮质的表征。\\t\\n',\n",
       " '\\n',\n",
       " '![image-20241122143343822](assets\\\\image-20241122143343822.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 12: HippoRAG结构示意。*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t该检索方法由两阶段构成，在**Offline Indexing**阶段，用微调后的LLM从语料库中抽取知识图谱三元组，将语料库中的重要信息以名词短语的形式提取，从而实现更细粒度的模式分离。提取出的三元组构成一个开放式的无约束的知识图谱，作为人工海马体索引。并通过检索编码器提供相似三元组检索功能，为下游的模式补全提供重要信息。通过提示工程的方式，我们可以从给定的文本中抽取出三元组，即开始实体、实体间的关系、结尾实体。然后依据这些抽取的实体构建成一个图，具体地，假设有$N$篇文章，为每一篇文章抽取知识三元组，对所有文章抽取完后共计得到$M$个实体，那么我们可以得到一个$M\\\\times N$的计数矩阵$A$，计数矩阵中的元素$a_{ij}$代表实体$e_i$在文章$P_j$出现的频次，而这个计数矩阵其实也就对应了一个图，我们可以将图写入到图数据库如$Neo4j$中，值得注意的是，在检索阶段我们需要从图中检索出和查询中的实体$c_i$相似的实体节点$e_j$，因此需要依赖到向量检索的功能，当实体数量非常庞大时，我们需要快速检索的功能帮我们从海量实体中快速的检索到相似的实体，而这一点$Neo4j$的$GDS$插件有提供向量检索功能，详情可阅读[此处](https://neo4j.com/labs/genai-ecosystem/vector-search/)。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Online Retrieval**阶段则模拟人类记忆检索过程，人工新皮质从$Query$\\u200b\\u200b中提取出命名实体，称作(Query Enamed Entities)，并利用检索编码器与知识图谱中的节点关联。这些被选中的节点称为查询节点，作为部分线索输入到人工海马体中，通过模式补全激活相关图谱邻域。此过程使用个性化PageRank（Personalized PageRank）算法，限制搜索范围在特定查询节点上，类似海马体从部分线索提取关联信号。最终，通过聚合PPR输出的节点概率，对之前索引的段落进行排序。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t具体地，快速向量检索将会检索出与查询实体$c_i$语义相似的实体节点$e_j(j=1,...,k)$，这些语义相似度的实体节点就相当于人脑在回忆时的一些线索，接下来人脑要进一步根据这些线索回忆起更完整的情景，换而言之，我们需要根据这些作为线索的实体节点，计算出完整的文本与这些线索的相似度并进行排序，这一功能则是通过个性化的PageRank算法实现。其背后的直觉如下：PageRank是一个衡量网页重要程度的算法，其核心思想是如果一个网页有很多指入的链接，说明这个网页在其他网页广为引用，表明该网页有较高的重要程度，反之如果一个网页指出的链接比较多则不能说明这个网页比较重要，同理，如果我们将这些实体节点视作一个个网页，指入当前节点的箭头数量多则说明当前节点的重要性高，则可以根据PageRank算法计算一个节点向量$V_e=(e_i,e_2,...,e_M)^T$。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t让我们回顾PageRank算法的具体细节，将互联网的网页视作节点，从该网页跳转到其他网页的链接视作有向边，用户继续浏览网页时会等概率地（随机）点击当前网页中指出的链接从而跳转到另一个页面，而随着用户点击次数地增加，这一整个长期地随机跳转就会构成一个稳定的模式，即马尔可夫平稳过程，每个网页的PageRank值就对应平稳分布中的某个概率。给定一个有向图:\\n',\n",
       " '\\n',\n",
       " '![image-20250121094837215](E:\\\\Study\\\\知识构建\\\\NLP系列\\\\assets\\\\image-20250121094837215.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 13: PageRank示意图。(自绘)*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t网页$A$有三条入边，网页$B$跳转到$A$和$C$，网页$C$跳转到$A$和$D$，网页$D$会跳转到$A,B,C$。可以将出边视作均匀地传递能量，则$A$可以获得$1/2B$和$1/2 C$，$B$可以获得全部的$D$，$C$可以获得$1/2B$和$1/3D$，$D$可以获得$1/2C$。PageRank是一个不断迭代的算法，上述的过程可以通过图13的右边的矩阵递推公式描述，而这个转移矩阵其实就对应了一个马氏链，随着时间$t$增加，最终的PageRank向量会趋于稳定，即得到马氏链的平稳分布：$\\\\pi=A\\\\pi$。PageRank算法具体流程在此就不多赘述了，具体细节读者可以阅读此处[参考](https://en.wikipedia.org/wiki/PageRank#Simplified_algorithm)。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t值得注意的是，并不是所有的马氏链都有平稳分布，马氏链存在平稳分布的充要条件为**非周期**与**不可约**，**非周期**的意思是任意状态出发经过有限步回到自身的最大公约数为1，即从状态$s_i$出发回到自身的步数不呈现周期性规律，如$2,4,6,8,....$就是周期性的，因为最大公约数是$2$。**不可约**(irreducible)一词源自于线性代数，即不可再分割的，无法化简的，因此其意思是马氏链中的任意两个状态经过有限步状态转移都能从一个状态转移到另一个状态，是彼此可达的。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t而在HippoRAG中，将实体视作网页通过PageRank算法求解实体的PageRank向量的原理不变，将与Query有关联的实体检索得到若干个候选实体$\\\\set{R_j}_{j=0}^{k}$，这些检索到的实体有对应的出边和入边，即其他实体和这些实体有关联。根据候选实体，我们可以利用个性化PageRank算法计算得到最终的PageRank向量$V_e=(e_i,e_2,...,e_M)^T$，而有了向量我们就可以利用向量空间模型计算实体向量与所有文档的相似程度，具体思想如下图：\\n',\n",
       " '\\n',\n",
       " '![image-20250121101832509](E:\\\\Study\\\\知识构建\\\\NLP系列\\\\assets\\\\image-20250121101832509.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 14 HippoRAG中文章相似度计算。（自绘）*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t文档和实体共现矩阵$C$的每一行代表文档$P_j$的文档向量，$C_{ji}$代表实体$e_i$在文档$P_j$中出现的频次，相似的文档则实体出现的频次或者实体交集数量较大，实体$e_i$\\u200b\\u200b\\u200b在该文档中出现频次较高则说明该实体比较重要。而PageRank算法计算的实体向量能衡量实体的重要程度，因此计算文档向量与实体向量的点积相似度可以很大程度代表二者的相关程度。而向量空间模型有其固有缺陷：如（1）语义信息缺失，无法考虑词的上下文顺序，无法理解多义词和同义词。（2）稀疏性，计算效率低，词语数量较大时向量维度高且稀疏，导致实际上语义相似的向量通过余弦或内积相似度计算时得到0的结果，即计算的相似度分布容易呈现出偏态和峰态。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t整体而言HippoRAG的特点如下：通过抽取文档的三元组知识构建知识图谱，存储文档中的关键信息构建索引。在检索时抽取查询中的实体$\\\\set{c_i}_{i=1}^{n}$，根据向量检索快速检索到图中与当前查询实体相关的候选实体节点$\\\\set{R_j}_{j=1}^{k}$，再通过个性化PageRank算法对整个图计算PageRank向量$V_e\\\\in\\\\mathbb R^{M \\\\times 1}$，为了体现出节点的特性，一个自然的想法是对这些候选实体节点赋予一些更高的权重，而不是让所有节点都具有同等的重要性，因此可以在初始化概率分布时给候选节点增加一些重要程度$s_i$，文中定义$s_i=\\\\frac{1}{|P_i|}$，即如果出现这个节点的文章数比较多则说明这个节点的重要程度较低，这和$TF\\\\text{-}IDF$\\u200b的思想一样。最后依据PageRank向量与向量空间模型中的文档向量进行相似度计算并排序，从而找到与当前查询最相关的文档。\\n',\n",
       " '\\n',\n",
       " '> [!NOTE]\\n',\n",
       " '>\\n',\n",
       " '> 在标注的PageRank算法中，增加初始分布某几个维度如$i,j,k$的概率并不能保证得到的平稳分布中的$i,j,k$维度的PageRank值一定是较大的，因为马氏链的平稳分布只与状态转移矩阵有关，不受初始化分布的影响，初始值只会影响收敛速度。而PageRank算法的变种如Personalized PageRank，初始分布可以影响最终的平稳分布。\\n',\n",
       " '\\n',\n",
       " '## 2.4 LLMs+Perona-Plug\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**Liu**[[17]]()等人认为通过检索式长期记忆的方式实现个性化智能能助手可能会无法完全利用上用户完整的对话信息，并且不能理解用户说话的风格导致影响对话效果。因此提出了一个轻量级的插件式的用户嵌入模块，通过构造每个用户的特定嵌入建模用户所有的对话历史上下文，在不微调大语言模型的参数下让大语言模型能更好地理解并捕捉用户偏好与习惯。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t该方法采用了一个插件式的用户嵌入模块，每个用户都有一个由共享用户嵌入器计算出的独特个性化嵌入。它将用户$u$的每一条历史行为$h_i^u$编码成一个密集向量$\\\\mathbf h_i^u$，并将这些嵌入向量根据当前输入$x_u$聚合为一个单一的个性化嵌入$\\\\mathbf P_u$。$\\\\mathbf P_u$会与大语言模型的嵌入层融再进行前向推理让LLM生成个性化响应。具体地，论文采用了一个轻量级的用户行为编码器进行用户行为编码：\\n',\n",
       " '$$\\n',\n",
       " '\\\\mathbf h_i^u=\\\\operatorname {{Enc}^{his}}(h_i^u) \\\\tag{11}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t用另一个编码器对用户当前输入$x_u$进行编码：\\n',\n",
       " '$$\\n',\n",
       " '\\\\mathbf x^u=\\\\operatorname {Enc^{input}}(x^u) \\\\tag{12}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t为了更好让大语言模型获得当前输入与用户行为历史间的表征信息，最直观的思想是通过注意力机制的方式让$\\\\mathbf x_u$与所有的$\\\\mathbf h_i$进行加权平均：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}\\n',\n",
       " 'w_{i} & =\\\\frac{\\\\exp \\\\left(\\\\mathbf{x}^{u \\\\top} \\\\mathbf{h}_{i}^{u}\\\\right)}{\\\\sum_{k} \\\\exp \\\\left(\\\\mathbf{x}^{u \\\\top} \\\\mathbf{h}_{k}^{u}\\\\right)} \\\\\\\\\\n',\n",
       " '\\\\mathbf{P}^{u} & =\\\\sum_{i} w_{i} \\\\cdot \\\\operatorname{Proj}\\\\left(\\\\mathbf{h}_{i}^{u}\\\\right),\\n',\n",
       " '\\\\end{aligned} \\\\tag{13}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t随后将$\\\\mathbf P_i$与大语言模型嵌入层融合用于引导大语言模型生成个性化回复，给定当前输入$x_u$和之前生成的内容$y_{<i}^u$，对应的损失函数定义如下：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}\\\\mathbf X_i^u&=[\\\\mathbf I;\\\\mathbf P^u;\\\\operatorname {Emb_{LLM}}(x^u);\\\\operatorname{Emb_{LLM}}({y_{<i}^u})] \\\\\\\\ \\n',\n",
       " '\\\\mathcal L&= -\\\\sum_u\\\\sum_i \\\\operatorname {log}p_{\\\\text{LLM}}(y_i^u|\\\\mathbf X_i^u) \\\\end{aligned}\\\\tag{14}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t其中，$\\\\operatorname{Emb_{LLM}(.)}$代表大语言模型的嵌入层，$\\\\mathbf I$是一个额外的可训练的指令嵌入层，用于捕获指令信息。在微调时只有三个部分的参数是可以改变的：$\\\\mathbf I$，输入编码器$\\\\operatorname {Enc^{input}}(.)$和投影层$\\\\operatorname {Proj}(.)$(两层MLP)。模型整体结构如下：\\n',\n",
       " '\\n',\n",
       " '![image-20241126114804400](assets\\\\image-20241126114804400.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 15: 模型结构。*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t该方法本质上还是微调，需要个性化的对话数据对三个模块参数进行调整，虽然其设计的注意力机制可以动态选择和当前输入最相关的历史行为，但本质上可以视作一个非常朴素的检索机制，只为与**当前输入**相关的历史行为信息分配较高的注意力权重，受编码器参数量影响，这一部分的用户历史行为信息融合得到的语义表征可能效果不那么好。此外，从工程落地的角度考虑，受算力限制，这套方案不太好实施，如对每一个用户而言，为其分配一个行为编码器和输入编码器是不现实的，且由于LLM的结构变动，无法直接套用主流的大语言模型加速推理框架，工程师需要做出较大改动。从可解释角度出发，也不像基于检索机制实现的个性化LLM易于理解，此外，仍然有一个对话窗口的问题需要解决，实际应用时模型不可能利用所有的历史对话信息作为前缀拼接，**因此该方法依然要考虑到与检索机制结合**。\\n',\n",
       " '\\n',\n",
       " '## 2.5 如何测评AI助手的长期记忆能力？\\n',\n",
       " '\\n',\n",
       " '### 2.5.1 Benchmark\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t虽然已有大量研究聚焦于如何更好地检索出相关片段提升大语言模型或智能助手在下游任务的表现，但对于如何评估AI助手在多轮交互对话的长期记忆能力的工作较少。[**LONGMEMEVAL[18]**](https://arxiv.org/abs/2410.10813)是一个能够全面评估智能助手长期记忆能力的基准，其涵盖了500个人工构建的高质量问题，用于测试五个关于长期记忆方面的核心能力：**信息抽取**、**跨会话推理**、**时间推理**、**知识更新**和**知识摈弃**。而为了全面评估智能助手的长期记忆能力，该论文构建了七种不同类型的题目，分别如下：\\n',\n",
       " '\\n',\n",
       " '```markdwon\\n',\n",
       " '单会话-用户: 测试回忆单个会话中用户提到信息的能力。\\n',\n",
       " '单会话-助手: 测试回忆单个会话中助手提到信息的能力。\\n',\n",
       " '单会话-偏好: 测试是否可以利用用户信息生成个性化回复的能力。\\n',\n",
       " '跨会话: 测试跨多个会话综合信息的能力。\\n',\n",
       " '知识更新: 测试识别用户个人信息变化并更新知识的能力。\\n',\n",
       " '时间推理: 测试对用户信息时态方面的意识，包括显式时间提及和时间戳元数据。\\n',\n",
       " '拒绝回答: 测试对涉及未知信息的问题不回答的能力\\n',\n",
       " '```\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t同时，**LONGMEMEVAL**提供了一个构造连贯、可拓展且附带时间戳信息的对话方法，基准具体的构建流程如下图所示[源自[18]]()：\\n',\n",
       " '\\n',\n",
       " '![image-20241204174359691](assets\\\\image-20241204174359691.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 15: LongMemEval数据生成流程。*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t论文首先定义了一个包含**164**个用户属性的本体，分为五大类：生活方式、个人物品、人生事件、情境背景与人口信息。然后利用大语言模型针对每一个属性生成以该属性为中心的用户背景段落，这些段落描述了用户的生活经历。从中随机抽取段落并再通过提示工程技术引导LLM生成若干个候选问题，并通过人工改写的方式确保这些问题的难度与多样性。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t接着，基于背景信息，人工将答案分解为一个或者多个证据语句($evidence\\\\text{ }statements$)，并为其合理地分配时间戳。接着，通过[Self-Chatting]()的方式构建任务导向的证据会话($evidence\\\\text{ }session$)，再将之前的证据语句插入到证据会话中。为确保数据的质量，以上步骤都经过人工干预确保准确性以及表述更加自然、口语化。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在对话历史构建阶段，从多个不相关的用户与AI的聊天中抽样，再将上一步构建的证据会话($ES_i$)插入到聊天($H_j$\\u200b\\u200b)中，并为user-AI chat sesion分配合理的时间戳。不相关对话的来源主要有两部分：（1）利用从用户背景中提取出的属性不冲突的事实通过Self-Chatting的方式进行生成。（2）公开发布的聊天数据，如ShareGPT、UltraChat等。通过这种方式可以最大程度的防止会话上下文的冲突性。\\n',\n",
       " '\\n',\n",
       " '### 2.5.1 长期记忆系统设计\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t论文将长期记忆视作一个大容量的键值对存储数据库$[(k_1,v_1),(k_2,v_2),...]$，其中$k_i$可以是异构的，$v_i$可以重复，并为记忆增强的智能助手制定了三个阶段：（1）$indexing$ 将每一个历史对话$(t_i,S_i)$转化成一个或多个键值对，（2）$retrieval$ 构建一个检索的查询并收集$k$个与查询最相关的键值对，（3）$reading$，LLM通过阅读检索到的结果生成答案。具体如下图所示[源自[18]]()。\\n',\n",
       " '\\n',\n",
       " '![image-20241205093622837](assets\\\\image-20241205093622837.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 16: 检索系统设计。*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**$value$** 表示长期记忆中每一个对话的格式和细粒度，user-AI 聊天会话通常比较冗长并涵盖多个主题，如果将一个聊天会话视作一个value则很可能会降低检索效率并不利于阅读。相反，如果将一个会话压缩成一个总结或者用户特定的事实则会导致信息损失，导致AI助手忽略一些细节，论文中比较了三种不同策略的$value$：**储存整个会话**、**将整个会话按照轮数分解**、**采用总结或事实抽取**。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t$key$ 会话信息被压缩或者分解后可能仍然会涵盖大量信息，但可能导致与用户查询的关联度降低，因此通常的做法还是用$value$本身做为$key$用于计算和$query$的相似度。论文引入了一个$key\\\\text{ }expansion$方法，即使用总结、关键短语、用户事实与时间戳用于增强索引，这种方式可以强调关键信息并能够通过多种途径有效检索。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t$query$ 对于普通类型的查询而言，上述提及的$key\\\\text{-}value$优化方式可能解决大部分检索问题，然而当查询涉及到时序信息时，简单的语义相似度或文本匹配度则可能不起效，因此论文设计了一个$time\\\\text{-}aware\\\\text{ }indexing$与$query\\\\text{ }expansion$策略，$value$用有时间戳的事件进行索引，然后在相关时间范围内进行检索。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t$reading$ 回答一个复杂的问题可能需要召回大量的记忆，虽然检索的准确度可以更具上述的设计进行优化，但是不能保证LLM能够从召回结果中有效地推理出正确答案，论文们探索了不同的阅读策略，并通过实验结果表明如在回答前提取关键信息([Chain of Note]())和使用结构化格式提示([structured format prompting]())能有效提高阅读能力。\\t![image-20241205104805902](assets\\\\image-20241205104805902.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 17: 问答效果对比实验。*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t图六[源自[18]]()实验结果表明，在不同设计的$value$中，将Round作为$value$能够提升问答效果。如果将Session Summary当作$value$，那么效果比较差，原因可能是因为总结压缩了过多的信息，丢失了很多细节，而将Round Facts当做$value$的效果也相当不错，token数量为Round的一半左右在多会话子集上的效果超过了Round，原因可能是Round Facts是篇幅较短关键的信息，本身就能够代表当前Round的绝大部分含义。这里有一个细节，即图中显示Round的token数量的范围较大，而Session的token数量范围较为集中，按理来说Session是由每一个Round构成的，二者的token数量应该相同，因此这里的Session应只是$evidence\\\\text{ }session$，而不是指整个历史聊天记录作为的session（```笔者已通过源码验证，读者可再自行确认```）。论文对不同的$key$的构造也进行了消融实验，具体如下图：\\n',\n",
       " '\\n',\n",
       " '![image-20250109201518199](assets\\\\image-20250109201518199.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 18: 不同key value模式组合实验结果。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t实验结果表明，在采用Round作为$value$的前提下$key=value+fact$的检索效果比$value+其他$的效果明显要高。在$value$是Session的前提下，$key=value+fact$的结果在大部分情况下比$value+其他$的下好。虽然fact，summary与keyprhase能够代表$value$绝大部分的含义，信息更加聚焦，但是单独使用这些关键信息或者压缩信息作为$key$用于索引的效果并不理想。为了帮助读者更清晰地了解其检索的运作机制以及存在的一些问题，笔者如下阐明其index expansion的过程，以round+userfact为例子。首先明确检索的文档为论文中所说的构建的history，history由三部分session构成：shareGPT,ultraGPT与evidence session，每一个history大约有200轮对话。笔者下载了论文项目提供的数据文件$\\\\text{longmemeval\\\\_s.json}$，数据列表中的每一个元素都是历史对话字典，字典中有一个键叫做haystack_session_ids，对应的值是一个存放不同session_id的列表，表明当前会历史会话由不同的session构成。笔者主要介绍抽取user fact的部分以及检索流程。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t抽取每一轮的user fact可以通过zero shot prompting或者few shot prompting的方式让大语言模型完成，具体步骤是将当前的evidence session作为prompt的变量传入，大语言模型再根据传入的对话列表抽取出一些相关的facts放在列表中，如果没有则列表为空。示意图如下：\\n',\n",
       " '\\n',\n",
       " '![image-20241211141927275](assets\\\\image-20241211141927275.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 19: user facts抽取示意。（自绘）*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t这些被抽取的usef fact将会被保存，用于检索时进行索引增强。而在检索完整的history时，只有evidence session部分会进行索引增强，其他由shareGPT与ultraGPT构成的对话由于没有facts，因此保持原有的样子。在index expansion时论文项目提供了几种不同的方案，如$separate$,$merge$,$replace$等，此处笔者给出$merge$方式的实现示意图：\\n',\n",
       " '\\n',\n",
       " '![image-20241211174746827](E:\\\\Study\\\\知识构建\\\\NLP系列\\\\assets\\\\image-20241211174746827.png)\\n',\n",
       " '\\n',\n",
       " '<center>图 20: Key Expansion示意。（自绘）</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t对于所有的$evidence\\\\text{ }session$，在$value$为round且$key=value+fact$时，将从中提取的所有的user facts与用户说的话user utterance进行拼接作为新的$key$，由于ultraGPT session与shareGPT session并不抽取user fact，因此$key$保留原样，即对应的用户说的话。有几点值得注意，如在通过提示工程抽取user facts时要注意描述的人称，最好给定几个案例让模型理解，否则直接通过Zero Shot Prompting方式抽取的user facts的描述一部分是以\"The user ...\"开头，一部分是\"I ....\"开头，如果$evidence\\\\text{ }session$对话轮数较多，从中抽取的facts数量也较大，那么key expansion后得到的$key$的内容也会较多。论文作者是先构建的$evidence\\\\text{ }session$再组合其他的session得到最终的history，如果是反过来，有用户大量的真实场景下的对话历史记录，那么如何定位对话历史中的$evidence\\\\text{ }session$\\u200b与抽取对话历史中的user facts需要更进一步的探索。\\n',\n",
       " '\\n',\n",
       " '## 2.6  现有记忆系统总结\\n',\n",
       " '\\n',\n",
       " '![image-20241205152504631](assets\\\\image-20241205152504631.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 20: 现有记忆系统模式。*</center>\\n',\n",
       " '\\n',\n",
       " '\\u200b\\tLONGMEMEVAL列举了现有的研究中的记忆系统并标明了不同索引构建细节。HippoRAG构建了一个以实体为中心的索引，而RAPTOR与Memwalker则通过递归摘要构建了一个分层索引。虽然更复杂的记忆索引结构可能对某些类型的查询有益，但它们也增加了在线交互中创建和维护索引的成本。具体来说，Flat类型Retrieval在需要添加新的记忆时，可以利用向量检索工具如Faiss/Milvus等，只需直接写入对应记忆的语义向量即可。而对于**HippoRAG**、**RAPTOR**和**Memwalker**而言，当新会话添加到记忆中时，需要对这些系统进行一定程度的重新索引，从而增加了计算开销。像ChatGPT类似的闭源商用智能聊天助手的在长期记忆方面的实现机制细节暂且未知，其官网提供了一篇推文[[19]](https://openai.com/index/memory-and-new-controls-for-chatgpt/)可让用户对其记忆管理有一定的了解，在管理界面的个性化中可以点击管理按钮查看并编辑用户的聊天记忆，可以从下图看到ChatGPT存储的记忆以facts的形式存在，笔者通过询问包含时间方面的信息验证了一下其是否有时间感知的能力。\\n',\n",
       " '\\n',\n",
       " '![image-20241209105121777](assets\\\\image-20241209105121777.png)\\n',\n",
       " '\\n',\n",
       " '![image-20241209110157622](assets\\\\image-20241209110157622.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 21:ChatGPT记忆系统。（自绘）*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t从聊天记录中可以看到，模型根据记忆中的facts回答十一月份和用户聊了哪些话题，但实际上用户在十一月份聊的话题与上述的facts无关，上图的facts其实是用户十二月聊的内容中提炼出来的，因此ChatGPT的记忆目前是无法感知时间的。综上，现有的关于记忆系统的研究仍未有一个成熟的解决方案。\\n',\n",
       " '\\n',\n",
       " '## 2.7 打造更好的AI助手\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**“懂我”**的终极形态是双向奔赴，即AI智能助手也能反过来主动向用户聊天，了解关心用户，这种模式超越了单向的命令和执行模式。AI不仅仅是一个响应者，它还应该能主动参与对话，甚至在适当时候引导参与者，通过主动开启对话，AI能够展现出对用户真诚的兴趣，这种主动性能够激发用户的参与感和新鲜感，从而保持用户对AI的长期兴趣。例如，如果AI知道用户在每周末喜欢阅读或进行户外活动，它可以在周末时主动提出相关的讨论话题或建议[[9]](https://mp.weixin.qq.com/s/zcSMerSKX30P2Hrwhoh0TA)。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t站在技术角度，结合上述提到的论文或相关工作，笔者整合了一个长期记忆机制：```如下只是笔者构想```\\n',\n",
       " '\\n',\n",
       " '### 2.7.1 长期记忆机制设计\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t首先是记忆存储，与Generative Agents种提到的不同层次的长期记忆类似，我们也可以设计不同层次的记忆，和Generative Agents不同，其记忆层次分为Memory Stream,Reflection,Planning and ReActing，其目的是让AI学会向人类一样思考与规划并做出合理的行动，但实际上这一套流程只能当作demo运行，不能直接应用到线上快速响应用户，因为Reflection、Planning 与ReActing的过程十分缓慢，而一般的线上AI陪伴服务也不需要智能体进行反思与规划。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t而人脑在存储记忆时是分层次的，这里我们只关注于人脑中的长期记忆，第二节开头所述人脑的长期记忆分为语义记忆和情景记忆。语义记忆存储关于世界的一般的知识，而情景记忆则存储个人经历和特定事件的记忆。同时，人脑对记忆的感知又是比较模糊的，大脑在处理和存储记忆时，会对信息进行**提炼、筛选和概括**，以便更高效地管理和利用记忆资源。人脑在存储和回忆信息时会根据**时间跨度**和**重要性**进行分层处理，长期记忆倾向于保留概括性信息，例如我们对于昨天、上周、上个月或更远的记忆，大脑逐步抹去细节，保留大致的情节或总结。如我们可能记得昨天开了个会议，但是不记得会议中每一个人说的每一句话，我们记得上周工作很忙，但是不记得上周每天都在忙的具体事项有哪些，上个月我们完成了一个公司中的重大项目，但是不记得每一周都遇到了哪些困难踩了哪些坑等细节。这种层次化和抽象化的处理有助于大脑有效地管理记忆资源。因此在不同层次的长期记忆设计上我们可以考虑分层次存储，比如第一层记忆粒度较细，就是用户每天的对话总结，第二个层次的记忆是每周的事件总结，如用户本周遇到了什么事情，心情如何。第三个层次的记忆可以是每月的事件总结，概述当月大致发生的重要事件。\\n',\n",
       " '\\n',\n",
       " '#### 2.7.1.1 记忆层次\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t人脑的长期记忆有两种，分别是语义记忆和情景记忆，语义记忆存储世界的一般知识，情景记忆则是个人经历与事件的记忆，[PerLTQA[20]]()旨在探索个性化的长期记忆在问答系统中的重要性和不同记忆类型的本质，其将个人资料与社交关系视作语义记忆，事件与对话视作情景记忆，并通过记忆分类、记忆检索与记忆综合三个子任务评估LLM的记忆利用能力。下图展示了笔者构造的长期记忆结构，在每日总结、每周总结与每月总结的基础上还有情景记忆与语义记忆。\\n',\n",
       " '\\n',\n",
       " '![image-20241213094232033](assets\\\\image-20241213094232033.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 22: 不同层次记忆。（自绘）*</center>\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t笔者认为PerLTQA的方式值得借鉴，在原有的三层记忆基础上加上语义记忆和情景记忆有一定必要性。在语义记忆方面，可以提取用户的兴趣爱好、职业、社交关系等信息形成一个关于用户的动态的画像，并作为system prompt中的变量传入，用户的语义记忆可以让大语言模型了解用户固有的一些特性，可以根据用户基本特性提供个性化的建议方案或是对话风格，这一点在khanmigo的个性化教学方面有充分体现，khanmigo的AI教学功能就是通过提示工程技术在system prompt中赋予用户的兴趣爱好实现个性化的教学举例功能。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t而情景记忆则是由两方面构成，一是用户的每日对话，二是从用户对话中抽取出的user facts。受LLM上下文窗口的限制，传入LLMs的对话历史不可能无限长，此外，较长的上下文也会导致[\"Lost in the middle\"[21]](https://arxiv.org/abs/2307.03172)现象（模型在中间部分的信息提取能力较弱，导致对位于输入中间的信息表现出明显的遗忘现象。），因此选择一个合适大小的历史对话窗口的同时，再从用户历史对话中检索出相关的对话片段放入system prompt中让LLM了解用户相关的经历实现个性化的回复是较好的方案。之所以抽取user facts是为了在检索阶段进行$key\\\\text{ }expansion$，即索引增强，其效果在上文介绍的LONGMEMEVAL的工作中有充分体现。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在记忆存储/构建索引时$key$与$value$如何选择合适？这一点我们同样可以借鉴LONGMEMEVAL的研究，在$value=round$,$key=value+fact$与$value=session,key=value+fact$两种模式的对比中我们可以发现，检索效果是前者要明显的低于后者，而问答上效果上则是前者明显高于后者，从实际问答效果出发，我们可以选择每一轮对话作为$value$，$key$为对应的$value+fact$构建索引。而在抽取user facts的时候可能会遇到一个问题，因为我们并不是针对当天对话的每一轮都进行$key\\\\text{ }expansion$，而是只对$evidence\\\\text{ }session$部分进行拓展，因此还需要从较长的对话历史中确定$evidence\\\\text{ }session$的范围，再从中抽取user facts，当对话历史较长时可能还需要分批次进行处理。如下笔者列举了构建索引的两种方案：\\n',\n",
       " '\\n',\n",
       " '|        |       key        | value |                   特点                   |\\n',\n",
       " '| :----: | :--------------: | :---: | :--------------------------------------: |\\n',\n",
       " '| 方案一 | value+user facts | round | 提升检索性能，抽取user facts过程较为繁琐 |\\n',\n",
       " '| 方案二 |      value       | round |                   简单                   |\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在实际场景中，每几轮对话就有可能蕴含较有价值的$userfacts$，以一天的聊天记录为例，我们可以将对话分轮次拆开先后进行用户事实抽取，比如以每10轮聊天对话进行抽取，如下笔者给出一个具体的提示工程案例：\\n',\n",
       " '\\n',\n",
       " '![image-20250109142430526](assets\\\\image-20250109142430526.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 23: 抽取user facts提示词示例。（自绘）*</center>\\t\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t即每10轮对话以变量的形式传入，由大语言模型抽取出对话中有价值的用户事实，并附带对应的轮次定位。将多轮对话传入是因为在一些情况下，有价值的用户事实可能需要经过多轮对话内容才能推理出来，更多的上下文有助于模型更精准地判断有价值的用户事实，而附带上用户事实的对应轮次信息则能用于定位与后继分析，下图为一个对应的例子和模型抽取结果：\\n',\n",
       " '\\n',\n",
       " '![image-20250109195025474](assets\\\\image-20250109195025474.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 24: key Expansion与Value Reconstruction重构示意。（自绘）*</center>\\t\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t可以看到，图中左侧的例子告诉大语言模型的输出要以字典的形式输出，而右边的抽取结果也表明模型成功判断了用户事实及对应的对话轮数。此外，有一点注意，原论文中实验结果表明$value=session$时问答任务上的效果要优于$value=round$时的效果，这可能是因为$value=round$时将信息切分得过细而丢失了关键片段，而在检索任务上则是后者的模式要优于前者，因此，我们可以考虑一个混合的方法，即$value=merged\\\\text{ }round$,$key=round$，具体如下：在存储$value$时，我们依据抽取的用户事实对应的$turn\\\\_range$将连续的几轮对话合并作为$value$，如上图24右侧的结果中，第$7-9$连续的三轮对话合并作为一个新的$value$，这样可以包含更多的信息，提升模型问答效果。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在长期记忆存储方面，我们可以先选择MongoDB数据库存储半结构化的对话数据。每日对话数据我们可以创建一个collection叫做$Daily\\\\text{ }conversation$存储每一轮的用户和助手对话。而每日对话总结、每周对话总结与每月对话总结可以再分别建立三个collection专门存储，可以通过后台定时任务如celery完成总结任务，如每天凌晨一点进行对话总结，每周一凌晨一点进行每周对话总结，每月1号凌晨一点进行上个月的每月对话总结，最后写入数据时附带上对应的时间戳信息即可。同时采用一个预训练的编码器将文本数据转化成语义向量写入对应的向量数据库如Milvus或者Faiss方便后继向量检索。\\n',\n",
       " '\\n',\n",
       " '#### 2.7.1.2 记忆检索\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t记忆检索是为了从过往的情景记忆中检索出相关片段，在检索时可以考量语义相关性，时效性、事件本身重要性或其他信息，从而更精确地检索到与当前查询最相关的记忆片段，帮助智能体更准确地理解用户过往经历。可以先通过语义召回的方式检索出$k$个相关的情景记忆，再通过重排序机制从$k$个情景记忆中选取$m(m<k)$个最终的记忆。在Generative AI[11]、Hou[15]、Du[20]等人的研究中均提供了较为合理与新颖的记忆检索方法，笔者认为上述的方案其实已经能应付大多数场景了，但是抱着杠精的思想，其实不难发现，在GenerativeAI与Hou的工作中，前者的赋分方式为简单的加权平均，若超参数$\\\\alpha,\\\\beta,\\\\gamma$设置任意一个不合理就可能会造成较大影响，如$\\\\alpha$过大且时间过近时有可能导致召回出较多不相关片段，若$\\\\beta$过大则会只召回语义类似的片段可能丢失潜在的重要信息，$\\\\gamma$\\u200b过大则导致召回一些明面上很重要但与当前查询无关的片段。Hou的工作中没有考虑到事件的重要程度，Du的工作在对召回的记忆片段计算记忆所属类别概率时额外要求较大的计算资源。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**记忆选择**人脑在回忆某件事情时检索的**语义记忆**和**情景记忆**的数量没有固定的上限或具体数值，而是受到多种认知和生理因素的影响，**认知科学研究**显示，人类短期内可以同时激活大约**5到9个概念**（即米勒的“7±2法则”），但大脑在长时间内可以动态地检索和切换更多的语义记忆节点。对情景记忆而言，在一次回忆中，人们通常能详细回忆大约**4到7个关键细节**，之后逐步唤起额外的细节。但大脑往往会聚焦于**几个关键片段**，并动态地选择要检索的细节[[22]](https://chatgpt.com/share/6762650a-b728-8004-a16b-1b849c2f6985)。笔者认为可以将动态规划中的经典的01背包问题用于记忆的筛选，具体细节如下：![image-20241216144452540](assets\\\\image-20241216144452540.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 23: 记忆选择与背包问题。（自绘）*</center>\\t\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t从最简单的场景考虑，每一条记忆只有语义相似度$s_i$和时间$l_i$两条属性，假设每个用户有一个容量有限的记忆槽，能放入的记忆有限，我们要找出如何让记忆槽的利用价值最大。可以很容易联想到最大价值背包问题，假设背包能装的物品最大重量为$W$，记忆的重量是$s_i$，价值是$l_i$，那么问题变成如何选择记忆使得装入背包的记忆价值最大，这是一个经典的01背包问题。即给定$N$个物品，每个物品有重量$weight[i]$和价值$value[i]$（对应时间$l_i$和语义相似度$s_i$），给定一个容量为$W$的背包，如何从所有物品中选择若干个物品使得放入背包的物品总价值最大且不超过背包容量$W$。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t定义一个二维数组$dp$，$dp[i][j]$表示从前$i$个物品中选择，且背包容量限制为$j$的情况下的最大价值。解决动态规划问题的最重要一步就是明确状态转移方程（递推式），是否选择第$i$个物品分析如下：（1）不选择，说明当前背包的最大价值和只考虑前$i-1$个时相同，故此时$dp[i][j]=dp[i-1][j]$，（2）选择，说明当前背包的容量是能将物品$i$放下的，容量限制为$j$时背包的剩余容量是$j-weight[i]$，加上当前物品价值$value[i]$，有$dp[i][j]=dp[i-1][j-weight[i]]+value[i]$，综合考虑有如下递推式：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}dp[i][j] = \\\\max(dp[i-1][j],dp[i-1][j-weight[i]]+value[i]) \\\\end{aligned} \\\\tag{15}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t上述是一个经典的01背包问题，我们可以拓展到每一个记忆有语义相似度$s_i$、时间$l_i$、记忆重要程度$m_i$三个属性，这样问题变成了一个二维背包问题，二维背包问题是：每个物品$i$有两个约束条件，比如重量$weight[i]$与体积$volume[i]$（对应时间$l_i$和事件重要程度$m_i$，时间越久远则$l_i$越大，事件越不重要则$m_i$越大），背包的限制为最大重量$W$与最大体积$V$，从所有物品中选择若干个物品使得放入背包的物品总价值最大且不超过背包容量$W$与体积$V$。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t我们可以定义一个三维的动态规划数组$dp[i][j][k]$表示从前$i$个物品中选择，且重量限制为$j$，体积限制为$k$的情况下的最大价值，同样，对于第$i$个物品我们分析如下：（1）不选择，说明当前背包最大价值和只考虑前$i-1$个时相同，$dp[i][j][k]=dp[i-1][j][k]$，（2）选择，如果$j>=weight[i]$且$k>=volume[i]$，则有$dp[i][j][k]=dp[i-1][j-weight[i]][k-volume[i]]+value[i]$，综合考虑则有如下递推式：\\n',\n",
       " '$$\\n',\n",
       " '\\\\begin{aligned}dp[i][j][k] = \\\\max(dp[i-1][j][k],dp[i-1][j-weight[i]][k-volume[i]]+value[i]) \\\\end{aligned}\\\\tag{16}\\n',\n",
       " '$$\\n',\n",
       " '\\u200b\\t上述情况还可以继续扩展，而随着维度的增加，时间复杂度会显著上升。关于如何根据检索出的记忆得到对应的物品重量$weight[i]$和体积$volume[i]$，则可以构造函数得出，以最近性$l_i$为例（$l_i$是自变量，$weight[i]$是因变量），时间越近则$weight[i]$越小，反之越大，此外有界和单调递增是函数必须满足的性质，我们可以很容易联想到函数$\\\\operatorname{sigmoid}(x)=\\\\frac{1}{1+\\\\exp(-x)}$满足这样的性质，同理，事件的重要程度到$volume[i]$我们可以用函数$\\\\operatorname{sigmoid}(-m_i)$\\u200b进行映射，如上便是一个简易的记忆选择方案。\\n',\n",
       " '\\n',\n",
       " '### 2.7.2 离线规划\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t规划的目的是为了实现与智能助手的“双向奔赴”，这一步可以采用离线方案，目的是为了实现与用户的双向互动，比如为了让AI展示出主动性，可以通过提示工程让大语言模型根据用户和智能体当天不同层次的记忆提出三个不同的问题，并设定一个时间让智能体在第二天主动发出关于这个问题的请求等。而与先前的记忆存储与记忆检索相比，离线规划场景更为聚焦，即长期记忆系统可以无缝迁移，但是如何离线规划则取决于垂直领域场景。\\n',\n",
       " '\\n',\n",
       " '#### 2.7.2.1 目标是什么？\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t关于离线规划什么？必须先明确目标，如在心理疏导场景下让智能助手主动发起话题，让用户感受到智能助手能主动地关心，建立更加稳固的关系，或是更精准地疏导用户。\\n',\n",
       " '\\n',\n",
       " '![image-20241216182019372](assets\\\\image-20241216182019372.png)\\n',\n",
       " '\\n',\n",
       " '<center>*图 24: 这是图片的图例描述。（自绘）*</center>\\t\\t\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**场景一：**用户找智能助手进行心理疏导，为了实现主动式对话，可以为用户的每一轮回复进行情绪分析，对用户负面情绪等级从$0-10$进行打分，若用户连续$M$轮内的负面情绪总分相加超过预先定义的阈值，则可以触发主动式聊天机制。此外，也可以依据用户和智能助手的每日聊天总结，离线规划出一个大纲，再反过来指导智能助手第二天要主动和用户发起合适的话题。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t**场景二：**一个孩子端与家长端的对话场景，孩子与智能助手1聊天（智能助手1身份是一个语言温暖，共情能力十足的朋友），家长与智能助手2（智能助手2身份是一位有多年教育经验、精通心理学的老师）聊天。孩子和家长再日常生活中总是会遇到矛盾的，尤其是青春期，这个时期的孩子大多充满叛逆、情绪波动较大，不易理解家长良苦用心，而家长往往感到困惑和无力，不知道如何与孩子有效沟通。场景二下的的$planning$功能则为家长与孩子的沟通建立起一座隐式的桥梁，如孩子和智能助手1吐槽父母对自己太苛刻，$planning$模块根据每天孩子与智能助手1的聊天了解到孩子与父母的相处现状，为此指定了一套详细的方案并告知智能助手2并由其转告父母。\\n',\n",
       " '\\n',\n",
       " '## 2.8 本章小结\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t本章节梳理了个性化智能助手的长期记忆系统，从最开始的Memory Bank到最后的LongMemEval的研究都一定程度聚焦于如何更好地检索到和用户查询跟相关的记忆片段，例如，在检索阶段，计算记忆相似度时附加上时间近效性，或是考虑到事件的重要程度以提高检索质量，此外，HippoRAG借鉴了人脑长期记忆系统，采用不同的索引设计模式，使其能更好的解决多跳推理的问题。这些研究为设计个性化智能助手长期记忆机制提供了宝贵的参考价值，然而，仅仅优化记忆的检索能力并不足以打造真正“懂我”的智能助手。智能助手还需具备更深入的用户理解能力，如感知用户情绪、了解个性化喜好，并能够主动发起对话。只有当助手展现出更“拟人化”的交互特性，才能与用户建立更持久的互动与情感连接。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t让智能助手和用户实现”双向奔赴“不单是一个技术性的问题，同时还依赖于应用场景和产品定位。如在教育场景和心理场景下，用户对智能助手的需求是不一样的，前者的用户群体可能更希望得到精准的知识追踪能力，智能助手根据用户习题反馈了解到用户在某些知识点方面掌握不全，从而进行跟个性化的学习资源推荐或者学习路径规划，后者的用户群体可能希望得到更共情的心理疏导与引导，这种能力依赖于大语言模型的规划能力，比如大语言模型根据用户画像和对话上下文进行思考，先总结出用户可能面临的问题，再进行任务拆解，并执行。智能助手在与用户的长期交互中，不可避免地会面临信息不足、推理错误或理解偏差的问题。反思机制使其能够在对话过程中不断调整自身的行为。例如，当智能助手发现某个建议未能有效帮助用户时，可以基于用户的反馈进行自我评估，调整对话策略，并在后续交流中提供更符合用户需求的建议。\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t此外，大语言模型的规划与博弈论密切相关，特别是在多阶段决策、动态调整策略和与其他智能体交互的场景中。博弈论为我们提供了分析和优化决策过程的工具，尤其是在不确定性和冲突的情况下。大语言模型在规划过程中，尤其是在复杂的多轮任务中，也面临着类似的决策挑战，比如如何选择最优策略、如何处理多个可能的未来情境，以及如何考虑到不同参与者的反应。未来的研究可以进一步探索在某领域下某应用场景下，如何结合强化学习、博弈论理论技术提升大语言模型的规划能力，使个性化智能助手更具智能性和可持续性。\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '# 参考文献\\n',\n",
       " '\\n',\n",
       " '[[1]The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864v3)\\n',\n",
       " '\\n',\n",
       " '[[2]Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log.](https://lilianweng.github.io/posts/2023-06-23-agent/.)\\n',\n",
       " '\\n',\n",
       " '[[3]**Chain**-of-**thought** prompting elicits reasoning in large language models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)\\n',\n",
       " '\\n',\n",
       " '[[4]**Tree** of **thoughts**: Deliberate problem solving with large language models](https://proceedings.neurips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html)\\n',\n",
       " '\\n',\n",
       " '[[5]**Least**-to-**most** prompting enables complex reasoning in large language models](https://arxiv.org/abs/2205.10625)\\n',\n",
       " '\\n',\n",
       " '[[6]ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)\\n',\n",
       " '\\n',\n",
       " '[[7]Reflexion: Language agents with verbal reinforcement learning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html)\\n',\n",
       " '\\n',\n",
       " '[[8]Chain of Hindsight Aligns Language Models with Feedback](https://arxiv.org/abs/2302.02676)\\n',\n",
       " '\\n',\n",
       " '[[9]十问“AI陪伴”：现状、趋势与机会](https://mp.weixin.qq.com/s/zcSMerSKX30P2Hrwhoh0TA)\\n',\n",
       " '\\n',\n",
       " '[[10]MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ojs.aaai.org/index.php/AAAI/article/view/29946)\\n',\n",
       " '\\n',\n",
       " '[[11]Generative Agents: Interactive Simulacra of Human Behavior](https://dl.acm.org/doi/abs/10.1145/3586183.3606763)\\n',\n",
       " '\\n',\n",
       " \"[[12]It's About Time: Incorporating Temporality in Retrieval Augmented Language Models](https://arxiv.org/abs/2401.13222)\\n\",\n",
       " '\\n',\n",
       " '[[13]Human-inspired Perspectives: A Survey on AI Long-term Memory](https://arxiv.org/abs/2411.00489)\\n',\n",
       " '\\n',\n",
       " '[[14] A Neurocognitive Model of Advertisement Content and Brand Name Recall.](https://pubsonline.informs.org/doi/abs/10.1287/mksc.1060.0212)\\n',\n",
       " '\\n',\n",
       " '[[15]\"My agent understands me better\": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents](https://dl.acm.org/doi/abs/10.1145/3613905.3650839)\\n',\n",
       " '\\n',\n",
       " '[[16]HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models](https://arxiv.org/abs/2405.14831)\\n',\n",
       " '\\n',\n",
       " '[[17]LLMs + Persona-Plug = Personalized LLMs](https://arxiv.org/abs/2409.11901)\\n',\n",
       " '\\n',\n",
       " '[[18]LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813)\\n',\n",
       " '\\n',\n",
       " '[[19]Memory and new controls for ChatGPT](https://openai.com/index/memory-and-new-controls-for-chatgpt/)\\n',\n",
       " '\\n',\n",
       " '[[20]PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering]()\\n',\n",
       " '\\n',\n",
       " '[[21]Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)\\n',\n",
       " '\\n',\n",
       " '[[22]GPT聊天询问](https://chatgpt.com/share/6762650a-b728-8004-a16b-1b849c2f6985)\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d1bd07-7a7c-431a-b0a0-247f700c3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=\"st.markdown('{title}')\\n\"\n",
    "content=\"st.markdown('{content}')\\n\"\n",
    "image=\"st.image('{img_path}')\\n\"\n",
    "latex=\"st.latex(r'''{formula}''')\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8015724-6d22-4143-8075-422a6699bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_str=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0641ed2-8059-4fc4-82c9-bbfb0062af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repalce_str(text:str):\n",
    "    text=text.replace(\"\\\\text{\",\"\\\\\\\\text{\")\n",
    "    text=text.replace(\"\\\\begin\",\"\\\\\\\\begin\")\n",
    "    text=text.replace(\"\\\\end\",\"\\\\\\\\end\")\n",
    "    text=text.replace(\"\\\\top\",\"\\\\\\\\top\")\n",
    "    text=text.replace(\"\\\\theta\",\"\\\\\\\\theta\")\n",
    "    text=text.replace(\"\\\\approx\",\"\\\\\\\\approx\")\n",
    "    text=text.replace(\"\\\\neq\",\"\\\\\\\\neq\")\n",
    "    text=text.replace(\"\\\\frac\",\"\\\\\\\\frac\")\n",
    "    text=text.replace(\"\\\\big\",\"\\\\\\\\big\")\n",
    "    text=text.replace(\"\\\\arg\",\"\\\\\\\\arg\")\n",
    "    text=text.replace(\"\\\\bar\",\"\\\\\\\\bar\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b7d6690d-bd19-4eac-be8d-11375519eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Learning to Rank\\n',\n",
       " '\\n',\n",
       " '\\u200b\\t在信息检索和推荐系统领域，排序问题始终是核心任务之一。从搜索引擎返回的网页列表，到电商平台为用户推荐的商品，排序算法无处不在。为了更智能、更个性化地进行排序，**Learning to Rank（学习排序）** 应运而生。Learning to Rank 的起源可以追溯到 2000 年代初期，随着机器学习在自然语言处理和信息检索中的广泛应用，人们逐渐意识到传统的基于规则或启发式的排序方法难以应对复杂的用户需求。2005 年，微软亚洲研究院发表了著名的 $\\\\text{RankNet}$（基于神经网络的排序学习模型），随后又推出 $\\\\text{LambdaRank }$和 $\\\\text{LambdaMART}$，这些工作开启了用监督学习方法直接优化排序的新时代。排序方法整体可分为Point-Wise,Pair-Wise,List-Wise三种，本文接下来讲按照顺序介绍文档排序场景下这三种方法的思想与具体细节。\\n',\n",
       " '\\n',\n",
       " '# 1.Point-Wise\\n',\n",
       " '\\n',\n",
       " '\\u200b\\tPoint-Wise Ranking 是学习排序的一类方法，它把排序任务视为 **回归或分类问题**。因此通常采样$\\\\text{BCE Loss}$或者$\\\\text{Focal Loss}$作为策略，以文档排序的场景为例，我们用BERT作为Cross Encoder捕获查询和文档间细粒度的语义交互，给定一个查询$query_i$、相关的文档$doc_i^+$（这里笔者假定只有一个相关文档，实际上可以有多个）和对应的$m$个候选文档$doc_{ij}^-,j=1,\\\\ldots,m$，将$query_i$和对应的文档通过特殊符号$\\\\text{[CLS][SEP]}$拼接后作为BERT的输入，由可训练的线性层$\\\\mathbf W$映射后再经过$\\\\operatorname{sigmoid}$函数得到对应的分数$s_i$，对于正样本的得分$s_i$，应该越接近$1$越好，对于负样本得分$s_j$，应该越接近$0$越好。如下图：\\n',\n",
       " '\\n',\n",
       " '![image-20250701180409168](assets/image-20250701180409168.png)\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1022e92b-8b8b-48ed-b067-07fb9820e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_str=\"\"\n",
    "meet_dollar=0\n",
    "formula=\"\"\n",
    "pattern = r'(assets/.*?\\.png)'\n",
    "for q in str_list:\n",
    "    if q.startswith(\"#\"):\n",
    "        t=title.format(title=q.strip())\n",
    "        write_str+=t\n",
    "        continue\n",
    "    elif q.startswith(\"\\u200b\"):\n",
    "        c=content.format(content=q.strip().replace('\\t',\"\"))\n",
    "        c = repalce_str(c)\n",
    "        write_str+=c\n",
    "        continue\n",
    "    elif q.startswith(\"$$\"):\n",
    "        meet_dollar+=1\n",
    "        if meet_dollar==1:\n",
    "            continue\n",
    "        elif meet_dollar==2:\n",
    "            formula+=q.strip()\n",
    "            write_str+=latex.format(formula=formula.replace(\"$$\",\"\"))\n",
    "            meet_dollar=0\n",
    "            formula=\"\"\n",
    "        continue\n",
    "    elif q.startswith(\"!\"):\n",
    "        match = re.search(pattern, q)\n",
    "        if match:\n",
    "            write_str+=image.format(img_path=match.group(1).strip())\n",
    "    elif q.startswith(\"<center>\"):\n",
    "        write_str+=\"图像\"\n",
    "    else:\n",
    "        if meet_dollar==1:\n",
    "            if q!=\"$$\":\n",
    "                formula+=q.strip()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e1a84f0-13be-4ca2-bbb2-5c51ccf205aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st.markdown('# Learning to Rank')\n",
      "st.markdown('​在信息检索和推荐系统领域，排序问题始终是核心任务之一。从搜索引擎返回的网页列表，到电商平台为用户推荐的商品，排序算法无处不在。为了更智能、更个性化地进行排序，**Learning to Rank（学习排序）** 应运而生。Learning to Rank 的起源可以追溯到 2000 年代初期，随着机器学习在自然语言处理和信息检索中的广泛应用，人们逐渐意识到传统的基于规则或启发式的排序方法难以应对复杂的用户需求。2005 年，微软亚洲研究院发表了著名的 $\\\\text{RankNet}$（基于神经网络的排序学习模型），随后又推出 $\\\\text{LambdaRank }$和 $\\\\text{LambdaMART}$，这些工作开启了用监督学习方法直接优化排序的新时代。排序方法整体可分为Point-Wise,Pair-Wise,List-Wise三种，本文接下来讲按照顺序介绍文档排序场景下这三种方法的思想与具体细节。')\n",
      "st.markdown('# 1.Point-Wise')\n",
      "st.markdown('​Point-Wise Ranking 是学习排序的一类方法，它把排序任务视为 **回归或分类问题**。因此通常采样$\\\\text{BCE Loss}$或者$\\\\text{Focal Loss}$作为策略，以文档排序的场景为例，我们用BERT作为Cross Encoder捕获查询和文档间细粒度的语义交互，给定一个查询$query_i$、相关的文档$doc_i^+$（这里笔者假定只有一个相关文档，实际上可以有多个）和对应的$m$个候选文档$doc_{ij}^-,j=1,\\ldots,m$，将$query_i$和对应的文档通过特殊符号$\\\\text{[CLS][SEP]}$拼接后作为BERT的输入，由可训练的线性层$\\mathbf W$映射后再经过$\\operatorname{sigmoid}$函数得到对应的分数$s_i$，对于正样本的得分$s_i$，应该越接近$1$越好，对于负样本得分$s_j$，应该越接近$0$越好。如下图：')\n",
      "st.image('assets/image-20250701180409168.png')\n",
      "st.markdown('​Point-Wise 把排序问题当作 **独立的回归或分类任务** 来做，预测每个样本的分值或概率。但排序真正关心的是 **文档之间的相对顺序**（比如NDCG、MAP、MRR 等），Point-Wise 并没有直接针对这些指标优化，因此即便模型预测的分值接近真实分值，也可能导致最终的排序顺序完全错误。此外，Point-Wise损失函数通常不能反映“局部排序错误”的严重程度，如把排名第$1$的文档得分预测稍低一些，导致其拍到了后几位，损失函数依然非常小，但是上线后用户体验和位置有关的衡量指标都很差。如果训练集中有大量负样本，模型可能只学会输出低分来降低损失，即便是类别加权的损失也难以将模型改进到正常水平。')\n",
      "st.markdown('# 2.Pair-Wise')\n",
      "st.markdown('## 2.1 RankNet& lambda Rank')\n",
      "st.markdown('​$\\mathrm{RankNet}$[[1]](https://icml.cc/Conferences/2015/wp-content/uploads/2015/06/icml_ranking.pdf)的核心思想是使用 **成对比较（pairwise approach）** 来学习一个排序函数，该函数可以根据文档对$(doc_i,doc_j)$的相关性预测它们相对于查询$q$的排序顺序。$\\mathrm{RankNet}$ 的损失函数可以表示为[[2]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)：')\n",
      "st.latex(r'''\\begin{align}C=\\frac{1}{2}\\left(1-S_{i j}\\right) \\sigma\\left(s_{i}-s_{j}\\right)+\\log \\left(1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}\\right)\\end{align}''')\n",
      "st.markdown('​对于给定的查询$Q$，$S_{ij}\\in\\{−1,0,1\\}$取值如下：$S_{ij}=1$：如果 $doc_i$ 比 $doc_j $更相关；$S_{ij}=0$：如果 $doc_i$ 和 $doc_j$ 相关性相同；$S_{ij}=−1$：如果 $doc_i$ 比更不相关。$s_i$ 和$s_j$分别表示文档$doc_i$和$doc_j$的相关性评分。$\\sigma$是一个超参数，用于缩放$s_i-s_j$​的值。')\n",
      "st.markdown('​当$S_{ij}=1$时有：')\n",
      "st.latex(r'''\\begin{aligned}C=\\log \\left(1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}\\right)\\end{aligned}''')\n",
      "st.markdown('​当$S_{ij}=0$时有：')\n",
      "st.latex(r'''\\begin{aligned}C=\\frac{1}{2}\\sigma\\left(s_{i}-s_{j}\\right)+\\log \\left(1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}\\right)\\end{aligned}''')\n",
      "st.markdown('​当$S_{ij}=-1$时有：')\n",
      "st.latex(r'''\\begin{align}C&=\\sigma\\left(s_{i}-s_{j}\\right)+\\log \\left(1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}\\right)\\\\&=\\log \\left(e^{\\sigma\\left(s_{i}-s_{j}\\right)})\\right)+\\log \\left(1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}\\right)\\\\&=\\log \\left(1+e^{\\sigma\\left(s_{i}-s_{j}\\right)})\\right)\\end{align}''')\n",
      "st.markdown('​$\\sigma=1$时的损失函数图像如下（自变量为$s_i-s_j$）：')\n",
      "st.image('assets/image-20250702163213780.png')\n",
      "st.markdown('​假设$s_i=\\mathbf x_i^{\\\\top}\\mathbf w,s_j=\\mathbf x_i^{\\\\top}\\mathbf w$，$\\mathbf w\\in \\mathbf R^{h\\times 1}$我们可以看一参数更新公式，以$w_k$（$\\mathbf w$的第$k$个分量）为例：')\n",
      "st.latex(r'''\\begin{aligned} \\frac{\\partial C(s_i,s_j)}{\\partial w_k}&= \\frac{\\partial C}{\\partial s_i}\\frac{\\partial s_i}{\\partial w_k}+\\frac{\\partial C}{\\partial s_j}\\frac{\\partial s_j}{\\partial w_k}\\\\&=\\bigg(\\frac{1}{2}\\left(1-S_{i j}\\right) \\sigma+\\frac{-\\sigma e^{-\\sigma(s_{i}-s_{j})}}{1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}}\\bigg)\\frac{\\partial s_i}{\\partial w_k}+\\bigg(-\\frac{1}{2}\\left(1-S_{i j}\\right)\\sigma +\\frac{\\sigma e^{-\\sigma(s_{i}-s_{j})}}{1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}}\\bigg)\\frac{\\partial s_j}{\\partial w_k}\\\\&=\\sigma\\bigg(\\frac{1}{2}\\left(1-S_{i j}\\right) -\\frac{e^{-\\sigma(s_{i}-s_{j})}}{1+e^{-\\sigma\\left(s_{i}-s_{j}\\right)}}\\bigg)(\\frac{\\partial s_i}{\\partial w_k}-\\frac{\\partial s_j}{\\partial w_k})\\\\&=\\sigma\\bigg(\\frac{1}{2}\\left(1-S_{i j}\\right) -\\frac{1}{1+e^{\\sigma\\left(s_{i}-s_{j}\\right)}}\\bigg)(\\frac{\\partial s_i}{\\partial w_k}-\\frac{\\partial s_j}{\\partial w_k})\\end{aligned}''')\n",
      "st.markdown('​且我们可以发现：')\n",
      "st.latex(r'''\\frac{\\partial C}{\\partial s_{i}}=\\sigma\\left(\\frac{1}{2}\\left(1-S_{i j}\\right)-\\frac{1}{1+e^{\\sigma\\left(s_{i}-s_{j}\\right)}}\\right)=-\\frac{\\partial C}{\\partial s_{j}}''')\n",
      "st.markdown('​因此对应的梯度更新的公式为：')\n",
      "st.latex(r'''\\begin{aligned}w_k\\to w_k-\\eta\\:\\frac{\\partial C}{\\partial w_k}=w_k-\\eta\\left(\\frac{\\partial C}{\\partial s_i}\\frac{\\partial s_i}{\\partial w_k}+\\frac{\\partial C}{\\partial s_j}\\frac{\\partial s_j}{\\partial w_k}\\right)\\end{aligned}''')\n",
      "st.markdown('​损失函数的变化近似为：')\n",
      "st.latex(r'''\\delta C\\approx\\sum_{k}\\frac{\\partial C}{\\partial w_{k}}\\delta w_{k}=\\sum_{k}\\frac{\\partial C}{\\partial w_{k}}\\left(-\\eta\\frac{\\partial C}{\\partial w_{k}}\\right)=-\\eta\\sum_{k}\\left(\\frac{\\partial C}{\\partial w_{k}}\\right)^{2}<0''')\n",
      "st.markdown('​即梯度下降一定沿着损失函数减小的方向更新。每次更新都会让损失值降低。然而，初版的$\\mathrm{RankNet}$训练效率低下——每次处理一对文档就要更新一次模型，如一个查询有$100$个候选文档，那么两两配对比较就需要$\\\\begin{pmatrix} 100 \\\\ 2\\\\end{pmatrix}$个文档对，这样的计算开销过大。我们回顾上述公式$xxxx$，可以将左边一部分复杂的公式定义：')\n",
      "st.latex(r'''\\lambda_{ij}\\equiv\\sigma\\bigg(\\frac{1}{2}\\left(1-S_{i j}\\right) -\\frac{1}{1+e^{\\sigma\\left(s_{i}-s_{j}\\right)}}\\bigg)''')\n",
      "st.markdown('​这样损失函数对单个参数分量的梯度公式就变得清爽了：')\n",
      "st.latex(r'''\\begin{aligned} \\frac{\\partial C(s_i,s_j)}{\\partial w_k}&= \\lambda_{ij}(\\frac{\\partial s_i}{\\partial w_k}-\\frac{\\partial s_j}{\\partial w_k})\\end{aligned}''')\n",
      "st.markdown('​我们可以把$\\lambda_{ij}$想象成一个作用力，如果模型把本该靠前的文档$doc_i$排在了$doc_j$后面，那么$\\lambda_{ij}$就会产生一个力，将$s_i$和$s_j$推开。那么这个作用力是否可以叠加与抵消呢？如果我们找到所有的$\\lambda_{ij}$预先计算好这些作用力，那么就可以实现从“逐渐更新”到“批量累计更新”。考虑一个查询下所有的文档对，看看每个权重受到了 多大的推力，并将$w_k$的梯度贡献加起来，有：')\n",
      "st.latex(r'''\\delta w_k=-\\eta\\sum_{\\{i,j\\}\\in I}\\lambda_{ij}(\\frac{\\partial s_i}{\\partial w_k}-\\frac{\\partial s_j}{\\partial w_k})''')\n",
      "st.markdown('​现在，这个公式可以写成更加统一的形式：')\n",
      "st.latex(r'''\\begin{aligned}\\delta w_k&=-\\eta \\sum_i\\lambda_i(\\frac{\\partial s_i}{\\partial w_k})\\\\\\lambda_i&=\\sum_{j:\\{i,j\\}\\in I}\\lambda_{ij}-\\sum_{j:\\{j,i\\}\\in I}\\lambda_{ji}\\end{aligned}''')\n",
      "st.markdown('​意思是对于某个文档$doc_i$，先找到相关性不如它的那些文档$doc_j$，此时可以算出一个向上的叠加的推力即$\\\\begin{aligned}\\sum_{j:\\{i,j\\}\\in I}\\lambda_{ij}\\\\end{aligned}$，同时也会有其他相关性比$doc_i$高的文档，此时$doc_i$上会有一个向下的叠加的拉力即$\\\\begin{aligned}-\\sum_{j:\\{j,i\\}\\in I}\\lambda_{ji}\\\\end{aligned}$。更直观一点，给定$5$个文档，假定关系如下：')\n",
      "st.image('assets/image-20250703105259182.png')\n",
      "st.markdown('​那么针对每一个文档$doc_i$，需要计算的$\\lambda_{ij}$、$\\lambda_{ji}$与$\\\\frac{\\partial s_i}{\\partial w_k}$如下表：')\n",
      "st.markdown('​因此，对于一个查询所有的文档，算出他们两两之间的$\\lambda_{ij}$，根据公式算出累加梯度$\\lambda_i$，所有$\\lambda_i$计算完后再根据公式进行梯度更新，显著加速训练速度。即原始的训练方式是遍历所有的文档对$\\\\begin{pmatrix} 100 \\\\ 2\\\\end{pmatrix}$算$O(n^2)$次计算（计算开销小），每次遍历就执行一次梯度更新（计算开销大），有$n^2$次廉价计算加$n^2$次昂贵计算。而改进后有先遍历所有文档对算出$\\lambda_i$$O(n^2)$次计算（计算开销小），再执行$n$次梯度更新，为$n$次昂贵计算，因此将计算复杂度降低至了线性，显著降低计算开销，而这个为加速而生的$\\lambda$梯度，启发了研究者们：我们是不是可以绕开复杂的损失函数，直接去定义和优化梯度呢？')\n",
      "st.markdown('​答案是——可以的，那为什么要直接定义梯度？因为$\\mathrm{RankNet}$的优化目标只是成对损失函数，而衡量排序好坏的指标如$\\operatorname{NDCG},\\operatorname {MRR}$并不是简单的成对损失，因此优化成对损失并不能保证训练后的模型在这些衡量指标上的效果就一定更好。那能否直接优化这些指标呢？——答案是可以，不过很麻烦，因为这些指标的计算涉及到排序算子，排序是一个不可导的操作，没法计算损失函数的梯度并反向传播，需要找到一些可导近似函数进行优化。因此$\\mathrm{LambdaRank}$提出不显示定义损失函数而是直接定义梯度来训练神经网络，$\\mathrm{LambdaRank}$在$\\mathrm{RankNet}$的基础上对$\\lambda_{ij}$进行了改造，直接定义梯度为：')\n",
      "st.latex(r'''\\lambda_{i j}=\\frac{\\partial C(s_i-s_j)}{\\partial s_i}=-\\frac{\\sigma}{1+e^{\\sigma\\left(s_{i}-s_{j}\\right)}} \\cdot|\\Delta \\mathrm{NDCG}|''')\n",
      "st.markdown('​其中，$|\\Delta \\mathrm{NDCG}|$是交换$i$与$j$排名后$\\mathrm{NDCG}$发生的变化，此时我们不再是让损失最小，而是要让向上的推力越大，使得模型预测的$\\mathrm{NDCG}$越大越好，因此更新参数时是梯度上升：')\n",
      "st.latex(r'''\\begin{aligned} w_k\\leftarrow w_k+\\eta\\frac{\\partial C}{\\partial w_k}=w_k+\\eta\\sum_i\\lambda_i\\frac{\\partial s_i}{\\partial w_k}\\end{aligned}''')\n",
      "st.markdown('​我们把$C$看作一个隐式收益，此时$C$的变化量近似为：')\n",
      "st.latex(r'''\\delta C\\approx\\frac{\\partial C}{\\partial w_k}\\delta w_k=\\eta\\big(\\frac{\\partial C}{\\partial w_k}\\big)^2\\gt 0''')\n",
      "st.markdown('​因此能说明这个隐式的收益说可以不断变大的。$\\mathrm{LmabdaRank}$除了可以优化$\\mathrm{NDCG}$指标，还能拓展到其他的指标如$\\mathrm{MAP},\\mathrm{MRR}$等。只要将$|\\Delta \\mathrm{NDCG}|$替换成对应的$\\mathrm{IR}$指标即可。')\n",
      "st.markdown('# 3.List-Wise')\n",
      "st.markdown('​listwise方法可以分为两类，位置有关的指标优化与位置无关的指标优化[[3]]()。和位置有关的衡量指标有$MRR$,$MAP$,$NDCG$等，而模型参数关于这些衡量指标不可导，我们通常采用函数近似的方式构造一个可导函数作为优化目标，从而实现模型参数的更新。首先，我们需要明确的是，什么衡量指标/算子是不可导的？')\n",
      "st.markdown('## 3.1不可导算子的可导近似')\n",
      "st.markdown('​学高等数学的时候我们知道导数是指函数描述的是函数在某一处的变化率，可导描述的就是指导数在某一处的变化率是否存在，常见的可到操作有：加减乘除、平方、对数、指数、线性变化、切片等。而不可导就是指函数在某些点处的导数不存在，或者不具备可微性，常见的不可导操作有：阶跃函数、$\\\\arg\\max$​、$\\max$、指示函数、排序、采样。')\n",
      "st.markdown('​在深度学习中，训练神经网络时由于策略的选择原因，标准的优化目标可能涉及到不可导算子，因此通常需要找一个可导算子进行近似。常见的可导算子不可导近似如下：')\n",
      "st.markdown('​以表格中的$\\max$不可导算子为例，$\\operatorname{max}$ 算子的作用是从一个向量中获得最大值，如$\\mathbf v=(2,3,4,1,4,5)^{\\\\top}$的最大值是$5$，则$\\max \\mathbf v=5$，其近似如下：')\n",
      "st.latex(r'''\\max(x_1,x_2,...,x_n)\\approx\\lim_{\\tau\\rightarrow \\infin}\\frac{1}{\\tau}\\log\\sum_{i=1}^{n}\\exp(\\tau x_i)''')\n",
      "st.markdown('​$\\tau$越大则近似越好，当$\\tau$取$1$时则$\\max$算子的近似就是$\\operatorname{log sum exp}$。$\\operatorname{sort}$算子和采样算子本文将会在接下来的章节详细介绍。')\n",
      "st.markdown('## 3.2 SoftRank')\n",
      "st.markdown('​$\\\\text{SoftRank}$[[3]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/SoftRankWsdm08Submitted.pdf)的思想是过文档的得分和排名进行概率建模，实现了对$\\\\text{NDCG}$等指标的可微近似，从而使得梯度下降等优化方法得以应用。$\\\\text{NDCG}$指标计算依赖于$\\\\text{DCG}$和$\\\\text{IDCG}$，但是$\\\\text{DCG}$这个指标中涉及到了$\\mathrm {sort}$的操作是不可导算子，因此训练时没法直接反向传播，如果将$\\\\text{DCG}$和$\\\\text{IDCG}$有一个平滑点的可导函数近似，那$\\\\text{NDCG}$自然也就可导了。给定神经网络预测的分数向量$\\mathbf s$，相关性标签$\\mathbf r$，$\\\\text{NDCG}@k(\\mathbf s,\\mathbf r)$计算方式如下：')\n",
      "st.latex(r'''\\begin{aligned} \\text{NDCG@}k(\\mathbf s,\\mathbf r)&=\\frac{\\text{DCG@}k(\\mathbf s,\\mathbf r)}{\\text{IDCG@}k(\\mathbf r)}\\\\\\text{DCG@}k(\\mathbf s,\\mathbf r)&=\\sum_{j=1}^{k}g(r_{\\pi^{-1}(j)})D(j)\\\\\\text{IDCG@}k(\\mathbf r)&=\\sum_{j=1}^{k}g(r_{\\pi_\\mathbf r^{-1}(j)})D(j)=\\max \\text{DCG@}k(\\mathbf s,\\mathbf r) \\end{aligned}''')\n",
      "st.markdown('​其中$g(\\cdot)$是增益因子，$g(z)=2^z-1$，$D(j)=1/\\log(j+1)$是位置折扣因子。$\\pi^{-1}(j)$是指排序$\\pi$中第$j$个位置对应的原来文档的索引，$\\pi$是神经网络预测的分数向量$\\mathbf s$对应的排序后的列表，$\\pi_{\\mathbf r^{-1}(j)}$是指相关性分数列表$\\mathbf r$从大到小排序得到排序$\\pi_{\\mathbf r}$中第$j$个位置对应的原来文档的索引。$\\\\text{IDCG@}k(\\mathbf r)$就是最大化$\\\\text{DCG@k}$，可以看到其和模型预测的分数是无关的，因此给定相关性标签$\\mathbf r$，$\\\\text{IDCG@}k(\\mathbf r)$可以被预先计算出。')\n",
      "st.markdown('​举例：假设当前有$5$个文档，对应的相关性标签$\\mathbf r=[0,1,2,0,3]$，神经网络预测的分数$\\mathbf s=[0.02,0.01,0.41,0.22,0.31]$，那么先计算$\\\\text{DCG@k}$再计算$\\\\text{IDCG@k}$。依据神经网络预测得分排序得到$[0.41,0.31,0.22,0.02,0.01]$，对应的$\\pi^{-1}=[3,5,4,1,2]$（假定索引从$1$开始），则对应的增益为：')\n",
      "st.markdown('​因此$\\\\text{DCG@}5(\\mathbf s,\\mathbf r)=\\\\frac{3}{1}+\\\\frac{7}{\\log_23}+\\\\frac{0}{\\log_24}+\\\\frac{0}{\\log_25}+\\\\frac{1}{\\log_26}$。$\\\\text{IDCG@}k(\\mathbf r)=\\\\frac{7}{1}+\\\\frac{3}{\\log_23}+\\\\frac{1}{\\log_24}+0+0$。$\\\\text{SoftRank}$将$\\\\text{DCG@k}$用概率近似得到$$\\\\text{SoftDCG@k}$$。具体地，假设当前查询$query$有$k$个候选文档集合$\\set{doc_j}_{j=1}^{k}$。将$query$与$doc_j$拼接后得到的文本对$\\mathbf x_j$送入一个$\\mathrm{Encoder}$，得到神经网络输出的$k$个分数$f(\\mathbf \\\\theta,\\mathbf x_j),j=1,..,k$，$\\\\text{SoftRank}$假设当前文本对$\\mathbf x_j$的输出分数$s_j$不再是确定的，而是服从于高斯分布：')\n",
      "st.latex(r'''\\begin{aligned} s_j \\sim \\mathcal N(s_j|f(\\mathbf \\theta,\\mathbf x_j),\\sigma_s^2)\\end{aligned}''')\n",
      "st.markdown('​如果给定两个文档$doc_i$与$doc_j$，从各自的高斯分布中采样得到的分数为$S_i,S_j$,我们想判断谁和$query$更加相似，那么就可以判断$S_i$与$S_i$谁大，但是由于分数是一个随机变量，因此我们看的是一个概率$P(S_i>S_j)$，即$\\mathrm{Pr}(S_i-S_j)>0$，而服从高斯分布的随机变量之差仍然是高斯分布，我们定义文档$i$打败文档$j$的概率$\\pi_{ij}$：')\n",
      "st.latex(r'''\\begin{aligned} \\pi_{ij} :=  \\operatorname{Pr}(S_i-S_j>0)=\\int_{0}^{\\infin} \\mathcal N(s|\\bar {s_i}-\\bar{s_j},2\\sigma_s^2) \\operatorname {d}s \\end{aligned}''')\n",
      "st.image('assets/learning2rank/Gaussian-area.png')\n",
      "st.markdown('​我们可以基于成对比较的方式来近似排序，其背后直觉如下：如果文档$doc_j$的排名比较靠后，说明其和其他文档在对比时都被打败了，具体地：假设共计$5$个文档，$doc_2$的排名为$4$（从$0$开始排），说明$doc_2$在和其他四个比较时都被打败了，如果其他文档$doc_j$打败$doc_2$的概率较大，则说明$\\pi_{i2},i\\\\neq2$较大，当$\\bar{s_i}-\\bar{s_2}$较大时高斯分布大于$0$部分的面积接近$1$，此时有$r_{doc_2}=4\\\\approx \\sum_{i=1,i\\\\neq 2}\\pi_{i2}$。因此任意文档$j$的排序$r_j$的期望可以表示如下：')\n",
      "st.latex(r'''\\begin{aligned} \\mathbb E\\big[r_j\\big]= \\sum_{i=1,i\\neq j}^N\\pi_{ij}=\\sum_{r=0}^{N-1}rP(r_j=r)\\end{aligned}''')\n",
      "st.markdown('​我们可以将上述式子看作一个$N-1$次的相互独立的伯努利实验，$r_j\\sim\\operatorname{Bernoulli}(\\pi_{ij}),j\\\\neq i$。但整体而言与标准的二项分布有所不同，标准的二项分布的概率质量函数有一个明确的解析式：')\n",
      "st.latex(r'''P(X=k)=\\begin{pmatrix} n \\\\ k\\end{pmatrix}p^k(1-p)^{n-k}''')\n",
      "st.markdown('​现在，文档$doc_j$的位置可以看成一个随机变量的期望，将$\\mathrm{DCG}$指标中的$D(r_j)$用$\\mathbb E\\\\big[D(r_j)\\\\big]$替代，则我们可以得到一个可导的计算指标$\\operatorname{SoftNDCG}$，即：')\n",
      "st.latex(r'''\\begin{aligned}DCG&=\\sum_{i=1}^{N}g(j)D(r_j)\\\\&\\approx \\sum_{i=1}^{N}g(j)\\sum_{r=0}^{N-1}D(r_j)P(r_j=r)\\end{aligned}''')\n",
      "st.markdown('​只要知道$P(r_j=k)$就能计算$\\mathrm{DCG}$，文档$j$的排序位置$r_j$的取值可能为$0,...,N-1$，但是我们会发现情况有点复杂，即$P(r_j=k)$的解析式表达起来很繁琐，当$r_j$取值为$0$时虽然有$P(r_j=0)=\\prod_{i=1,i\\\\neq j}^N(1-\\pi_{ij})$，但是当$r_j=1$时可能是$N-1$种情况，即:')\n",
      "st.latex(r'''P(r_j=1)=\\sum_{k=1,k\\neq j}^{N}\\big(\\pi_{kj}\\prod_{i=1,i\\neq j，i\\neq k}^{N}(1-\\pi_{ij})\\big)''')\n",
      "st.markdown('​当$r_j=2$时，则有$\\\\begin{pmatrix} N-1 \\\\ 2\\\\end{pmatrix}$种情况，$r_j=3$时有$\\\\begin{pmatrix} N-1 \\\\ 3\\\\end{pmatrix}$种情况，更一般的，我们可以将$P(r_j=k)$表达如下：')\n",
      "st.latex(r'''P(r_j=k)=\\sum_{\\substack{E\\subseteq\\{1,2,...,N\\} \\setminus \\{j\\}\\\\|E|=K}}^{}\\big(\\prod_{e\\in E}\\pi_{ej}\\prod_{i=1,i\\neq j，i\\neq e}^{N}(1-\\pi_{ij})\\big)''')\n",
      "st.markdown('​该概率质量函数虽然是解析式，但计算时需遍历子集，属于“非封闭形式”（因涉及到组合爆炸）。通常我们需要迭代进行求解，现在来思考一下不同视角下的$P(r_j=k)$的表达方式，从一开始，假设只有一个文档$doc_j$，那么第一次排序其排在位置$0$的概率必然是$1$，现在有第二个文档$doc_i,i\\\\neq j$进来，我们需要确认文档$doc_j$排在$0$还是$1$，这种情况下如果$doc_j$仍然排在$0$的概率是$1-\\pi_{ij}$，排在$1$的概率是从$0$位置跌落一名$\\pi_{ij}$。假设有第三篇文档进来，则文档$doc_j$的位置排名只可能出现排序不变及往下跌落一位的情况，不可能上升，如果我们将文档$doc_j$的排序位置视作一个状态，则这个状态只与前一个状态有关，且只可能保持不变或者由前一个状态转移到下一个相邻的状态（第$3$个时刻的位置$3$只能转移到第四个时刻的位置$3$或者位置$4$，不可能转移到位置$2$或位置$5$），整体情况如下图所示：')\n",
      "st.image('assets/learning2rank/position-transition.png')\n",
      "st.markdown('​用一个类似于状态转移矩阵的方式刻画（图中灰色圆圈代表文档处于该位置的概率为0），将$P^{(i)}_j(r=k)$记作排序$i,i=1,..j-1,j+1..,N$篇文档时，文档$doc_j$排在位置$k$的概率，则我们可以写一个递推公式：')\n",
      "st.latex(r'''P_j^{(i)}(r=k)=P_j^{(i-1)}(r=k-1)\\pi_{ij}+P_j^{(i-1)}(r=k)\\big(1-\\pi_{ij}\\big)''')\n",
      "st.markdown('​最终计算得到$P_j^{(N)}(r=k):=P(r_j=k)$，针对所有的$j=1,...,N$,我们都会利用上述公式进行迭代计算得到一个位置分布向量$\\mathbf P(r_j)=(P_j(0),P_j(1),$,$....,P_j(N-1))^{\\\\top}$，再基于公式x求得最终的$\\mathrm{SoftNDCG}$​​​，最终用如下公式作为损失函数进行反向传播：')\n",
      "st.latex(r'''\\begin{aligned} \\mathcal L(f;\\mathbf x,\\mathbf r)=1-\\frac{1}{Z_m}\\sum_{j=1}^{m}(2^r_j-1)\\sum_{k=1}^mD(k)P_j(r=k)\\end{aligned}''')\n",
      "st.markdown('## 3.3 $\\text{Approximate Rank}$ & $\\text{SmoothRank}$')\n",
      "st.markdown('​$\\\\text{Approximate Rank}$[[5]()认为$\\\\text{NDCG}$指标不连续的根本原因在于排序的位置关于排序的得分是一个不可导的映射，因此将排序位置用排序分数近似是一个非常直接的想法，具体地，$DCG=\\sum_{i=1}^{N}g(j)D(r_j)=\\sum_{i=1}^{N}g(j)/\\log(1+\\pi(\\mathbf x_j))$，而$\\pi(\\mathbf x_j)$是文档在按照模型预测的相关性分数排序后的列表中的位置，从分数到位置的这个操作是不可导的，我们可以把$\\pi(\\mathbf x_j)$用$s_j=f(\\\\theta,\\mathbf x_j)$进行近似：')\n",
      "st.latex(r'''\\begin{aligned}\\pi(\\mathbf x_j)&=1+\\sum_{i=1,i\\neq j}^N\\operatorname I\\set{s_i>s_j}\\\\&\\approx 1+\\sum_{i=1,i\\neq j}^NP(s_i-s_j>0)\\end{aligned}''')\n",
      "st.markdown('​其中指示函数$\\mathbf I$可以用概率近似，$\\\\text{Approximate Rank}$提出以如下方式近似$\\pi(\\mathbf x_j)$：')\n",
      "st.latex(r'''\\begin{aligned}\\hat \\pi(\\mathbf x_j)&=1+\\sum_{i=1,i\\neq j}^NP(s_i-s_j>0)\\\\&=1+\\sum_{i=1,i\\neq j}\\frac{\\exp(-\\alpha(s_j-s_i))}{1+\\exp(-\\alpha(s_j-s_i))}\\end{aligned}''')\n",
      "st.markdown('​故最后的损失函数是：')\n",
      "st.latex(r'''\\begin{aligned}\\mathcal L(f;\\mathbf x,\\mathbf r)&=1-\\text{DCG}_{\\max}^{-1}\\sum_{j=1}^{N}g(j)/\\log(1+\\hat\\pi(\\mathbf x_j))\\end{aligned}''')\n",
      "st.markdown('​$\\\\text{SmoothRank}$[[6]]()的思想与$\\\\text{Approximate Rank}$类似，都是基于近似$\\pi(\\mathbf x_j)$的思想，区别在于近似时的概率质量函数不同，$\\\\text{SmoothRank}$的具体近似公式如下：')\n",
      "st.latex(r'''\\begin{aligned}&\\sum_{j=1}^{m}\\sum_{u=1}^{m}g(y_{\\pi^{-1}(u)})D(u)\\mathbf I\\{x_j=x_{\\pi^{-1}(u)}\\}\\\\&=\\sum_{u=1}^{m}g(y_{\\pi^{-1}(u)})D(u)\\sum_{j=1}^{m}\\mathbf I\\{x_j=x_{\\pi^{-1}(u)}\\}\\end{aligned}''')\n",
      "st.markdown('​其中$\\mathbf I\\{x_j=x_{\\pi^{-1}(u)}\\}$为指示函数，当文档$x_j$排在位置$u$时才为$1$，否则为$0$。指示函数的近似公式如下：')\n",
      "st.latex(r'''\\begin{aligned}h_{ju}=\\frac{e^{-(f(x_j)-f(x_{\\pi^{-1}(u)}))^2/\\sigma}}{\\sum_{k=1}^{m}e^{-(f(x_k)-f(x_{\\pi^{-1}(u)}))^2/\\sigma}}\\end{aligned}''')\n",
      "st.markdown('​有了$h_{ju}$便能定义平滑版本的$\\\\text{NDCG}$指标，并定义损失函数如下：')\n",
      "st.latex(r'''\\begin{aligned} \\mathcal L(f;\\mathbf x;\\mathbf r)&=1-\\sum_{j=1}^{m}\\sum_{u=1}^{m}g(y_{\\pi^{-1}(u)})D(u)\\mathbf I\\{x_j=x_{\\pi^{-1}(u)}\\}\\\\&=1-\\sum_{u=1}^{m}g(y_{\\pi^{-1}(u)})D(u)\\sum_{j=1}^{m}\\mathbf I\\{x_j=x_{\\pi^{-1}(u)}\\}\\\\&=1-\\sum_{u=1}^{m}g(y_{\\pi^{-1}(u)})D(u)\\sum_{j=1}\\frac{e^{-(f(x_j)-f(x_{\\pi^{-1}(u)}))^2/\\sigma}}{\\sum_{k=1}^{m}e^{-(f(x_k)-f(x_{\\pi^{-1}(u)}))^2/\\sigma}}\\end{aligned}''')\n",
      "st.markdown('​值得注意的是$\\\\text{DCG}_{\\max}^{-1}$不见了，且与$\\\\text{Approximate Rank}$不同的是——不直接近似位置带入$D(u)$，而是乘在外面作为$g(y_{\\pi^{-1}(u)})D(u)$的系数，可能是因为')\n",
      "st.image('assets/learning2rank/SmoothRank.png')\n",
      "st.markdown('​从图像中可以看到，如果$r$比较小时，随便波动带来的误差较大，当$r$比较大时，对误差的波动就相对没那么敏感了。即折扣函数对排名靠前的位置更敏感，而对靠后位置的贡献衰减较慢但变化幅度较小。因此直接近似位置可能导致在文档数量较多时误差累积增大，显著改变整体$\\\\text{DCG@}k$的值。而$\\\\text{DCG}_{\\max}^{-1}$是一个常数，和神经网路具体预测的值无关，可以预先计算，因此加不加上并不影响优化目标。')\n",
      "st.markdown('## 3.4 ListNet&ListMLE')\n",
      "st.markdown('​ListNet[[7]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf)将排序视作一个概率分布，用交叉熵损失优化排序网络。具体地，ListNet先介绍了排列概率$\\\\text{(Permutation Probability)}$，假设$\\pi$是一个关于$n$个物品的排列，$\\Phi$是一个严格单调增函数，给定一个分数列表$\\mathbf s$，则排列$\\pi$出现对应的概率定义为：')\n",
      "st.latex(r'''P_{\\mathbf s}(\\pi):=\\prod_{j=1}^n\\frac{\\Phi(\\mathbf s_{\\pi(j)})}{\\sum_{k=j}^{n}\\Phi(\\mathbf s_{\\pi(k)})}''')\n",
      "st.markdown('​其中$\\mathbf s_{\\pi(j)}$是排序$\\pi$中位置$j$的得分，假设有一个文档$\\set{1,2,3}$的分数列表$\\mathbf s=\\set{s_1,s_2,s_3}$，则排序$\\pi=\\langle2,3,1\\rangle$出现的概率：')\n",
      "st.latex(r'''\\begin{aligned}P_{\\mathbf s}(\\pi)=\\frac{\\Phi(s_2)}{\\Phi(s_2)+\\Phi(s_3)+\\Phi(s_1)}\\frac{\\Phi(s_3)}{\\Phi(s_3)+\\Phi(s_1)}\\frac{\\Phi(s_1)}{\\Phi(s_1)}\\end{aligned}''')\n",
      "st.markdown('​上述公式只是一个定义式，因为客观世界里排列实际出现的概率和分数列表没有关系，如假设有三个水果分别是“桃子、梨子、苹果”，实际的排列情况有$6$种可能性，每一个排列是等概率的。但是假设将这个水果的排列交给一个人来排列，则有可能人会根据自己的喜好将偏爱的水果放在前面，如果让一个人来排列多次这三个水果，很有可能是喜好的水果在前的次数较多，而排列概率模拟了人类排列物品的过程，但人为定义的排列概率是否满足概率分布的要求我们还需要证明，文中给出了引理$2$：')\n",
      "st.markdown('​给定两个长度皆为$n$分数列表$\\mathbf s_1,\\mathbf s_2$，则我们可以计算得到两个排列分布向量$\\\\begin{pmatrix}\\pi_{\\mathbf s_1}^1\\cdots\\pi_{\\mathbf s_1}^{n!}\\\\end{pmatrix}^\\\\top$与$\\\\begin{pmatrix}\\pi_{\\mathbf s_2}^1\\cdots\\pi_{\\mathbf s_2}^{n!}\\\\end{pmatrix}^\\\\top$。我们可以用一个度量概率分布差异的指标作为损失函数。在实际计算中，由于排列$n$个物品有$n!$种可能性，计算过于复杂，因此我们只考虑物品$j$被排在第一个位置的概率——Top1 Probability。ListNet定义物品$j$被排序在第一个位置的概率公式：')\n",
      "st.latex(r'''\\begin{aligned}P_{\\mathbf s}(j)=\\sum_{\\pi(1)=j,\\pi\\in\\Omega_n}P_{\\mathbf s}(\\pi)\\end{aligned}''')\n",
      "st.markdown('​我们希望分数越大的物品被排在第一个位置的概率越高，只要计算每一个物品被排在第一个位置的概率$P_{\\mathbf s}(k),k=1,...,n$。但即便如此，根据公式计算概率也几乎不可能，因为直剩下的$n-1$个物品排序仍然有$n-1!$种可能性，文中的定理$6$则明确告诉我们可以通过如下式子计算$P_{\\mathbf s}(j)$：')\n",
      "st.latex(r'''P_{\\mathbf s}(j)=\\frac{\\Phi(\\mathbf s_j)}{\\sum_{k=1}^n\\Phi(\\mathbf s_k)}''')\n",
      "st.markdown('​此外，我们仍需确保$P_{\\mathbf s}(j)$也是符合概率分布的，文中引理7：')\n",
      "st.markdown('​通过Top1概率，给定一个真实标签的概率分布$\\mathbf P_{\\mathbf r}^{(i)}$和模型输出的概率分布$\\mathbf P_{\\mathbf s}^{(i)}$，我们就可以用一个度量分布的指标作为损失函数，这里笔者沿用论文中的符号，查询$q^{(i)}$对应的候选文档集合为$\\mathbf d^{(i)}=\\set{d^{(i)}_{1},...,d_{n^{(i)}}^{(i)}}$，查询$q^{(i)}$对应文档集合的人工标记相关性分数向量记作$\\mathbf r^{(i)}=(r^{(i)}_{1},...,r^{(i)}_{n^{(i)}})$，模型预测的输出为$\\mathbf s^{(i)}=(s^{(i)}_{1},...,s^{(i)}_{n^{(i)}})$，我们看一下ListNet模型的损失函数：')\n",
      "st.image('assets/image-20250709153714737.png')\n",
      "st.markdown('​假设在标注阶段的每一个文档的相关性分数都是确切的，查询$q^{(i)}$标签的概率分布记作$\\mathbf P_{\\mathbf r}^{(i)}=(P_{r^{(i)}}(1),...,P_{r^{(i)}}(n))^{\\\\top}$，模型输出的概率分布记作$\\mathbf P_{\\mathbf s}^{(i)}=(P_{s^{(i)}}(1),...,P_{s^{(i)}}(n))^{\\\\top}$，前者是目标分布，后者是真实分布，我们可以找一个度量分布的函数作为损失函数，KL散度。若采用KL散度作为损失，则$\\operatorname{D}_{KL}(\\mathbf P_{\\mathbf r}^{(i)}||\\mathbf P_{\\mathbf s}^{(i)})$表达如下：')\n",
      "st.latex(r'''\\begin{aligned}\\operatorname{D}_{KL}(\\mathbf P_{\\mathbf r}^{(i)}||\\mathbf P_{\\mathbf s}^{(i)})&=\\sum_{k=1}^{n^{(i)}} P_{r^{(i)}}\\log \\frac{P_{r^{(i)}}}{P_{s^{(i)}}}\\\\&=C-\\sum_{k=1}^{n^{(i)}} P_{r^{(i)}}\\log {P_{s^{(i)}}}\\\\&=C+H(\\mathbf P_{\\mathbf r}^{(i)},\\mathbf P_{\\mathbf s}^{(i)})\\end{aligned}''')\n",
      "st.markdown('​由于标签是固定的，即$C$一直不变，采用KL散度作为损失函数等价于用交叉熵作为损失函数。所以$\\\\text{ListNet}$的损失函数就是Cross Entropy Loss，即：')\n",
      "st.latex(r'''\\begin{aligned} \\mathcal L_{\\mathrm{ListNet}}(f;\\mathbf x;\\mathbf r)&=-\\sum_{k=1}^{K}P_{r_k}\\log\\sum_{k=1}^{K}\\frac{\\exp(s_k)}{\\sum_{j=k}^{K}\\exp(s_j)}\\\\&=-\\sum_{k=1}^{K}\\frac{r_k}{\\sum_{j=1}^{K}r_j}\\log\\bigg(\\sum_{k=1}^{K}\\frac{\\exp(s_k)}{\\sum_{j=1}^{K}\\exp(s_j)}\\bigg)\\end{aligned}''')\n",
      "st.markdown('​$\\\\text{ListMLE}$[[8]](https://www.researchgate.net/publication/221345286_Listwise_approach_to_learning_to_rank_-_Theory_and_algorithm)则采用了一个更加直接的方式，以真实的标签顺序排列作为目标，基于极大似然估计的思想设计损失函数，即让某个序列出现的概率最大，给定模型预测的分数向量$\\mathbf s$，相关性标签$\\mathbf r$，$\\pi$是相关性标签从大到小排序，$\\mathbf s_{\\pi}$是模型预测分数按照相$\\pi$进行排序后的向量，那么损失函数可以写作：')\n",
      "st.latex(r'''\\mathcal L_{\\text{ListMLE}}(f;\\mathbf x;\\mathbf r)=-\\mathbb \\log\\prod_{k=1}^{K}\\frac{\\exp{s_{\\pi(k)}}}{\\sum_{j=k}^{K}\\exp(s_{\\pi(k)})}\\tag{3-x}''')\n",
      "st.latex(r'''-\\log P(\\hat {\\mathbf s})=-\\log \\frac{\\exp (4)}{\\exp (4)+\\exp (2)+\\exp (3)+\\exp (1)}\\frac{\\exp (3)}{\\exp (3)+\\exp (2)+\\exp (1)}\\frac{\\exp (2)}{\\exp (2)+\\exp (1)}''')\n",
      "st.markdown('​然而，$\\\\text{ListNet}$与$\\\\text{ListMLE}$这类排序模型的优化目标与位置无关，用IR的衡量指标如$\\\\text{NDCG}$来衡量排序好坏时有不一致的矛盾，因此有学者通过引入位置因子解决这个问题，如$\\\\text{P-ListMLE}$[[9]](https://dl.acm.org/doi/10.5555/3020751.3020798)。')\n",
      "st.markdown('## 3.5 Neural Sort&Neural NDCG')\n",
      "st.markdown('​在上述的ListWise形式的排序中，由于$NDCG$指标的计算关于神经网络的输出是一个不可导的操作，因此不可直接优化，可以通过函数近似替代的方式或者与位置无关的损失函数来优化网络，那有没有研究是找到一个离散的排序的可导近似呢？——NeuralSort[[10]](https://arxiv.org/abs/1903.08850)就是一种“连续松弛”，是排序操作的可导近似。')\n",
      "st.markdown('​Neural Sort的目标是通过反向传播的方式优化包含$\\operatorname{sort}$算子的优化目标，即如下形式：')\n",
      "st.latex(r'''\\begin{aligned} L(\\theta,\\mathbf s)=f(P_{\\mathbf z};\\theta)\\\\\\text{where } \\mathbf z=\\operatorname{sort}(\\mathbf s)\\end{aligned}''')\n",
      "st.markdown('​其中，$\\mathbf s\\in\\mathbb R^{n}$是一个$n$元实值向量，$\\mathbf z$是一个由$\\mathbf s$排序后的置换向量，$P_{\\mathbf z}$是一个置换矩阵。在上文中，笔者罗列了不可导算子的可导近似，其中$\\operatorname{sort}$算子的可导近似是$\\operatorname{Sinkhorn}$算子，$\\operatorname{Sinkhorn}$算子是一种 **将非可导的排序操作（如 permutation matrix）变成可导形式** 的方法，它常用于 **可微排序（differentiable sorting）** 或 **可微分配（differentiable assignment）** 的场景中。它的关键是将 **离散的置换矩阵（permutation matrix）** 近似为 **可导的双随机矩阵（doubly stochastic matrix）**。置换矩阵$P_{\\mathbf z}$是一个特殊的方阵，用于对向量或矩阵进行排序，一个$n\\times n$的矩阵$P$称之为置换矩阵，当且仅当：')\n",
      "st.latex(r'''\\sum_{j=1}^nP_{ij}=1\\text{\t}\\forall i,\\text{}\\sum_{i=1}^nP_{ij}=1\\text{ }\\forall j''')\n",
      "st.markdown('​给定一个$n$维的置换向量$\\mathbf z=(z_1,z_2\\cdots z_n)^{\\\\top}\\in\\mathbb R^n$，$z_i\\in\\set{1,2\\cdots n}$且两两不同，对应的置换矩阵$P_{\\mathbf z}$满足：')\n",
      "st.latex(r'''P_{\\mathbf z}[i,j]=\\left\\{\\begin{array}{ll}1 & \\text { if } j=z_{i}(\\text{排序中第 i 大的元素是原始的第 j 个元素}) \\\\0 & \\text { otherwise }\\end{array}\\right.''')\n",
      "st.markdown('​假设一个输入向量$\\mathbf s=(9,1,5,2)^{\\\\top}$，且**定义**$\\operatorname{sort}:\\mathbb R^n\\rightarrow\\mathcal Z_n$算子是一个将$n$维实值向量输入映射到一个降序排列的置换向量的操作，则$\\mathbf s$经过$\\operatorname{sort}$作用后对应的置换向量是$\\mathbf z=\\operatorname{sort}({\\mathbf s})=(1,3,4,2)^{\\\\top}$，即第$1$个元素第$1$大，第$3$个元素第$2$大，第$4$个元素第$3$大，第$2$个元素第$4$大，置换向量对应的置换矩阵$P_{\\mathbf z}$：')\n",
      "st.latex(r'''P_{\\mathbf z}=\\left[\\begin{array}{llll}1 & 0 & 0 & 0 \\\\0 & 0 & 1 & 0 \\\\0 & 0 & 0 & 1 \\\\0 & 1 & 0 & 0\\end{array}\\right],P_{\\mathbf z}\\cdot \\mathbf s=(9,5,2,1)^{\\top}''')\n",
      "st.markdown('​给定任意$\\mathbf s$我们需要先找其和到$P_{\\operatorname{sort}(s)}$明确的数学表达关系，我们知道的是$P_{\\operatorname{sort}(s)}[i,j]=1$一定代表排序后第$i$大的元素对应于原始索$j$，第$1$大可以用$\\max$，最小可以用$\\min$，但是第$i$大这个该如何通过数学公式描述？我们需要借用这样一个引理：')\n",
      "st.markdown('​这个引理的证明很简单：')\n",
      "st.latex(r'''\\begin{aligned} \\sum_{i=1}^{k} \\mathbf s_{[i]}&=\\sum_{i=1}^{k} \\mathbf s_{[i]}-\\lambda+\\lambda k \\\\&\\leq\\lambda k+\\sum_{i=1}^{k} \\mathbf \\max(s_{[i]}-\\lambda,0)\\\\&\\leq \\lambda k+\\sum_{i=1}^{n} \\mathbf \\max(s_i-\\lambda,0)\\end{aligned}\\tag{3-x}''')\n",
      "st.markdown('​当$\\lambda$比$ s_{[k]}$小时，$\\max$算子是生效的，当$\\lambda$等于$s_{[k]}$时，$\\max$算子不生效，有：')\n",
      "st.latex(r'''\\begin{aligned} \\lambda k+\\sum_{i=1}^{n} \\mathbf \\max(s_{i}-\\lambda,0)&=\\lambda k +\\sum_{i=1}^{n} \\mathbf s_{i}-\\lambda\\\\&=k\\mathbf s_{[k]}+\\sum_{i=1}^{k} \\mathbf s_{[i]}-s_{[k]}\\\\&=\\sum_{i=1}^{k} \\mathbf s_{[i]}\\end{aligned} \\tag{3-x}''')\n",
      "st.markdown('​更具体地，等式$(3-x)$成立的条件是$\\mathbf s_{[k]}\\leq \\lambda \\leq \\mathbf s_{[k+1]}$。通过控制$\\lambda$的大小，我们可以得到$\\mathbf s$的前$k$个最大值之和。而第$k$大的值$\\mathbf s_{[k]}$表明上可以通过下式得到：')\n",
      "st.latex(r'''\\begin{aligned}\\mathbf s_{[i]}&=\\sum_{i=1}^{k} \\mathbf s_{[i]}-\\sum_{i=1}^{k-1} \\mathbf s_{[i]}\\\\&=\\min _{\\lambda \\in\\left\\{s_{1}, s_{2}, \\ldots, s_{n}\\right\\}} \\lambda k+\\sum_{i=1}^{n} \\max \\left(\\mathbf s_{i}-\\lambda, 0\\right)\\\\&-\\big(\\min _{\\lambda'\\in\\left\\{s_{1}, s_{2}, \\ldots, s_{n}\\right\\}} \\lambda'(k-1)+\\sum_{i=1}^{n} \\max \\left(\\mathbf s_{i}-\\lambda' 0\\right) \\big)\\\\&=\\min_{\\lambda}F_{k}(\\lambda)-\\min_{\\lambda'}F_{k-1}(\\lambda')\\end{aligned}''')\n",
      "st.markdown('​我们会发现这是一个差分$\\min$运算，会让优化问题变得复杂，不便于接下来的推导。为此，我们必须想一种方式让优化目标只有一个$\\min$。等式$\\sum_{i=1}^{k} \\mathbf s_{[i]}=\\lambda k+\\sum_{i=1}^{n} \\mathbf \\max(s_{i}-\\lambda,0)$的成立条件是$\\mathbf s_{[k]}\\leq \\lambda \\leq \\mathbf s_{[k+1]}$，我们思考是否可以再构造一个优化目标使得$\\mathbf s_{[k-1]}\\leq \\lambda \\leq \\mathbf s_{[k]}，$这样就可以通过夹逼的方式强迫$\\lambda=\\mathbf s_{[k]}$，优化$\\lambda$使得目标最小就得到了最终的$\\mathbf s_{[k]}$。换个角度想，前$k$大其实等价于后$n-k+1$小，如果将$\\mathbf s$取负即令$\\mathbf t=-\\mathbf s$，则$\\mathbf t$的前$n-k+1$大等价于$\\mathbf s$的后$n-k+1$小等价于$\\mathbf s$的前$k$​大。如下是一个直观的例子：')\n",
      "st.image('assets/learning2rank/reverse-index.png')\n",
      "st.markdown('​因此可以同样使用引理2把$\\mathbf t$的前$n-k+1$大的和写成：')\n",
      "st.latex(r'''\\begin{aligned} \\sum_{i=1}^{n-k+1}\\mathbf t_{[i]}&=\\min_{\\lambda\\in\\mathbf t=-\\mathbf s}\\big[ \\lambda(n-k+1)+\\sum_{i=1}^{n}\\max(\\mathbf t_i-\\lambda,0)\\big]\\\\&st.\\mathbf t_{[n-k+1]}\\leq \\lambda \\leq \\mathbf t_{[n-k+2]}\\end{aligned}''')\n",
      "st.markdown('​再令$\\lambda=-\\lambda$​，则有：')\n",
      "st.latex(r'''\\begin{aligned} \\sum_{i=1}^{n-k+1}\\mathbf t_{[i]}&=\\min_{\\lambda\\in\\mathbf -t=\\mathbf s}\\big[ -\\lambda(n-k+1)+\\sum_{i=1}^{n}\\max(\\mathbf \\lambda-s_i,0)\\big]\\\\&st.\\mathbf t_{[n-k+1]}\\leq -\\lambda \\leq \\mathbf t_{[n-k+2]}\\equiv \\mathbf s_{[k]}\\geq\\lambda \\geq \\mathbf s_{[k-1]}\\end{aligned}''')\n",
      "st.markdown('​但本质上我们是想求得$\\lambda$，因此我们合并两个$\\\\arg\\min_{\\lambda}$使得$ \\mathbf s_{[k]}\\geq\\lambda \\geq \\mathbf s_{[k-1]}$与$\\mathbf s_{[k]}\\leq \\lambda \\leq \\mathbf s_{[k+1]}$同时成立，即$\\lambda^*=\\mathbf s_{[k]}$，最终合并两式和，有关于$\\lambda$的优化目标为：')\n",
      "st.latex(r'''\\begin{aligned} \\lambda^*=\\mathbf s_{[k]}&=\\arg \\min _{\\lambda \\in \\mathbf{s}}\\left(\\lambda k+\\sum_{i=1}^{n} \\max \\left(\\mathbf s_{i}-\\lambda, 0\\right)+\\lambda(k-1-n)+\\sum_{i=1}^{n} \\max \\left(\\lambda-\\mathbf s_{i}, 0\\right)\\right) \\\\&=\\arg \\min _{\\lambda\\in\\mathbf s}\\lambda(2k-1-n)+\\sum_{i=1}^{n}\\max(\\mathbf s_i-\\lambda,0)+\\max(\\mathbf \\lambda-\\mathbf s_i,0)\\\\&=\\arg \\min _{\\lambda\\in\\mathbf s}\\lambda(2k-1-n)+\\sum_{i=1}^{n}|\\mathbf s_i-\\lambda|\\\\&=\\arg \\max _{\\lambda\\in\\mathbf s}\\lambda(n+1-2k)-\\sum_{i=1}^{n}|\\mathbf s_i-\\lambda|\\end{aligned}''')\n",
      "st.markdown('​我们看这个等式，并把它展开：')\n",
      "st.latex(r'''\\begin{aligned} \\lambda^*&=\\arg \\max _{\\lambda\\in\\mathbf s}\\lambda(n+1-2k)-\\sum_{i=1}^{n}| s_i-\\lambda|\\\\&=\\arg \\max _{\\lambda\\in\\mathbf s}\\begin{pmatrix}s_1(n+1-2k)-\\sum_{i=1}^{n}|s_i-\\mathbf s_1| \\\\s_2(n+1-2k)-\\sum_{i=1}^{n}| s_i-\\mathbf s_2|\\\\\\vdots \\\\s_n(n+1-2k)-\\sum_{i=1}^{n}| s_i-\\mathbf s_n|\\end{pmatrix}\\\\&=\\arg \\max _{\\lambda\\in\\mathbf s}[(n+1-2k)\\mathbf s-\\mathbf A_i\\mathbb 1]\\end{aligned}''')\n",
      "st.markdown('​该式子的含义是遍历$\\lambda \\in \\mathbf s$使得$\\lambda(n+1-2k)-\\sum_{i=1}^{n}|\\mathbf s_i-\\lambda|$达到最大，如果抛去$\\\\begin{aligned}\\\\arg\\max_{\\lambda}\\\\end{aligned}$的下角标$\\lambda$，则：')\n",
      "st.latex(r'''\\begin{aligned} &\\arg \\max \\begin{pmatrix}s_1(n+1-2k)-\\sum_{i=1}^{n}| s_i-\\mathbf s_1| \\\\s_2(n+1-2k)-\\sum_{i=1}^{n}| s_i-\\mathbf s_2|\\\\\\vdots \\\\s_n(n+1-2k)-\\sum_{i=1}^{n}| s_i-\\mathbf s_n|\\end{pmatrix}\\\\&=\\arg \\max [(n+1-2k)\\mathbf s-\\mathbf A_i\\mathbb 1]\\end{aligned}''')\n",
      "st.markdown('​作用是找到该向量中分量值最大元素对应的索引。若结果是索引$i$，则说明$s_i=s_{[k]}$，即排序后第$k$大的元素来自于原始索引$i$，理清了这层关系，我们开始构造置换矩阵$P$。我们按照行进构造，即先找排序后第$1$大的元素对应与原始索引是多少，再以此类。选定第$1$行，从左往右，那么我们就判断$\\mathbf s_1=\\mathbf s_{[k]}$时的$k$是多少，则$P[1,k]=1$，第$k$行之外的其他行$P[i,1]=0,i\\\\neq k$。那$s_i$第几大我们要一一判断，从$k=1,2,...,n$，对于第$1$​列而言，只要：')\n",
      "st.latex(r'''\\begin{aligned} \\arg \\max \\begin{pmatrix}s_1(n+1-2k)-\\sum_{i=1}^{n}| s_i-s_1| \\\\s_2(n+1-2k)-\\sum_{i=1}^{n}| s_i- s_2|\\\\\\vdots \\\\s_n(n+1-2k)-\\sum_{i=1}^{n}| s_i- s_n|\\end{pmatrix}=1，\\text{for k in }1,2,...,n\\end{aligned}''')\n",
      "st.markdown('​便能说明$\\mathbf s_1=\\mathbf s_{[k]}$，因此，对于第$1$行我们可以写成：')\n",
      "st.latex(r'''\\begin{aligned}P[1,j]==\\left\\{\\begin{array}{ll}1 & \\text { if } j=\\arg \\max [(n+1-2)\\mathbf s-\\mathbf A_{\\mathbf s}\\mathbb 1] \\\\0 & \\text { otherwise }\\end{array}\\right.\\end{aligned}''')\n",
      "st.markdown('​同理，第二行我们可以写成：')\n",
      "st.latex(r'''\\begin{aligned}P[2,j]==\\left\\{\\begin{array}{ll}1 & \\text { if } j=\\arg \\max [(n+1-2\\times 2)\\mathbf s-\\mathbf  A_{\\mathbf s}\\mathbb 1] \\\\0 & \\text { otherwise }\\end{array}\\right.\\end{aligned}''')\n",
      "st.markdown('​以此类推，第$i$行我们可以写成的形式：')\n",
      "st.latex(r'''\\begin{aligned}P[i,j]==\\left\\{\\begin{array}{ll}1 & \\text { if } j=\\arg \\max [(n+1-2i)\\mathbf s-\\mathbf A_{\\mathbf s}\\mathbb 1] \\\\0 & \\text { otherwise }\\end{array}\\right.\\end{aligned}''')\n",
      "st.markdown('​当然，$\\\\arg\\max$算子是不可导的，$P$也是不可导的，$P$的每一行是一个$\\operatorname{one-hot}$向量，我们找一个可导近似来近似这个$\\operatorname{one-hot}$向量，最简单的，可以用算子$\\operatorname{softmax with temperature}$近似，即：')\n",
      "st.latex(r'''\\lim _{\\tau \\rightarrow 0^{+}} \\widehat{P}_{\\operatorname{sort}(\\mathbf{s})}[i,:](\\tau)=P_{\\operatorname{sort}(\\mathbf{s})}[i,:] \\quad \\forall i \\in\\{1,2, \\ldots, n\\}''')\n",
      "st.markdown('​温度系数$\\tau$越小则该分布越接$\\operatorname{one-hot}$向量，$\\tau$越大则越接近平稳分布。接下来我们再思考如何构造优化目标，在很多排序任务中，**目标排序是唯一的**，比如给定一个数字列表$[3,2, 1, 4]$，我们知道升序结果是 $[1, 2,3, 4]$。但在**模型学习排序函数**时，并不是直接输出这个固定结果，而是输出一个分数向量，然后间接地产生排序。一个分数向量可能对应多个潜在的排列结果，如果只用一个排列，那么信号太稀疏了，不够全面，网络在学习时可能会记住某个输入对应的一种排列情况而不是学到通用规律。而列举分数向量$\\mathbf s$​所有排列情况显然也不现实，因此我们需要采样，即从所有可能性结果中选出一部分具有代表性的排列，然后评估这些排序的表现，用这些反馈去优化分数向量生成器，从而提高神经网络的泛化能力。')\n",
      "st.markdown('​直接采样排列$z\\sim \\operatorname{Plackett-Luce}(\\mathbf s)$这个操作也是不可导的，最常见的解决方式之一是重参数化，将离散采样过程转化为**确定性函数+噪声扰动**，使梯度能通过连续变量传递而$\\operatorname{Gumbel Softmax}$就是代表性的重参数化方法。重参数化方法是处理如下优化目标的一种方法：')\n",
      "st.latex(r'''\\begin{aligned} L_{\\theta}=\\mathbb E_{z\\sim P_{\\theta}(z)}\\big[ f(z)\\big]\\end{aligned}''')\n",
      "st.markdown('​由于采样操作不可导，因此没有办法写一个精确的$L_{\\\\theta}$，而$z$从$p_{\\\\theta}(z)$中采样会失去关于参数$\\\\theta$​的梯度信息，重参数方法则将采样变化成“固定随机数 + 可导变换”的方式使得反向传播可以用于训练目标涉及到采样操作的神经网络。其具体数学形式如下：设一个随机变量 $z∼p(z∣θ)z$，其中 $\\\\theta$ 是模型参数，例如均值 $\\mu$、标准差 $\\sigma$等，目标关于参数的梯度为：')\n",
      "st.latex(r'''\\nabla_{\\theta}\\mathbb E_{z\\sim p_{\\theta}(z)}\\big[ f(z)\\big]''')\n",
      "st.markdown('​引入一个可以重参数化的随机变量$\\epsilon\\sim p(\\epsilon)$，使得：')\n",
      "st.latex(r'''z=g_{\\theta}(\\epsilon)''')\n",
      "st.markdown('​其中，$g$是一个确定性的可导分布，$\\epsilon$的分布和$\\\\theta$无关，则有：')\n",
      "st.latex(r'''\\begin{aligned}\\nabla_{\\theta}\\mathbb E_{z\\sim p_{\\theta}(z)}\\big[ f(z)\\big]&=\\nabla_{\\theta}\\mathbb E_{\\epsilon\\sim p(\\epsilon)}\\big[ f(g_{\\theta}(\\epsilon))\\big]\\\\&=\\mathbb E_{\\epsilon\\sim p(\\epsilon)}\\big[ \\nabla_{\\theta}f(g_{\\theta}(\\epsilon))\\big]\\end{aligned}''')\n",
      "st.markdown('​以常见的高斯分布为例，假设随机变量$z\\sim \\mathcal N(z;\\mu_{\\\\theta},\\sigma_{\\\\theta})$，令$z=\\sigma_{\\\\theta}\\epsilon+\\mu_{\\\\theta}$，其中$\\epsilon \\sim \\mathcal N(\\epsilon;0,1)$，则有：')\n",
      "st.latex(r'''\\begin{aligned}\\nabla_{\\theta}\\mathbb E_{z\\sim p_{\\theta}(z)}\\big[ f(z)\\big]&=\\nabla_{\\theta}\\mathbb E_{\\epsilon\\sim \\mathcal N(\\epsilon;0,1)}\\big[ f(\\sigma_{\\theta}\\epsilon+\\mu_{\\theta})\\big]\\\\&=\\mathbb E_{\\epsilon\\sim \\mathcal N(\\epsilon;0,1)}\\big[ \\nabla_{\\theta}f(\\sigma_{\\theta}\\epsilon+\\mu_{\\theta})\\big]\\end{aligned}''')\n",
      "st.markdown('​在离散情况，将随机变量$z$用$y$代替，以从类别分布中采样为例：')\n",
      "st.latex(r'''\\mathbf P_{\\theta}=[P_{\\theta1},P_{\\theta2},...,P_{\\theta k}]''')\n",
      "st.markdown('​现在$y\\sim \\operatorname{Categorical(\\mathbf P_{\\\\theta})}$，我们需要找一个确定性的可导分布$y=g_{\\\\theta}(\\epsilon)$使得采样的随机性转移到随机变量$\\epsilon$上，而$\\operatorname{Gumbel Max}$提供了一种从类别分布中采样的方法[[11]](https://kexue.fm/archives/6705)（本节暂不对此进行深入的原理解析）：')\n",
      "st.latex(r'''\\begin{aligned} \\arg\\max_i &\\big(\\log  p_{\\theta i} -\\log(-\\log\\epsilon_i)\\big)^k \\\\\\epsilon_i &\\sim U[0,1]\\end{aligned}''')\n",
      "st.markdown('​即先从均匀分布中先采样得到$k$个随机数，然后再计算每一个$\\\\big(\\log  p_{\\\\theta i} -\\log(-\\log\\epsilon_i)\\\\big)^k$，找到最大值对应的索引：')\n",
      "st.latex(r'''\\arg\\max_i\\begin{pmatrix}\\big(\\log  P_{\\theta 1} -\\log(-\\log\\epsilon_1)\\big)^k\\\\\\big(\\log P_{\\theta 2} -\\log(-\\log\\epsilon_2)\\big)^k\\\\\\vdots \\\\\\big(\\log  P_{\\theta k} -\\log(-\\log\\epsilon_k)\\big)^k\\end{pmatrix}''')\n",
      "st.markdown('​前面讲过$\\\\arg\\max$算子不可导，通常用算子$\\operatorname{softmax with temperature}$近似，因此得到了$\\operatorname{Gumbel Max}$的可导近似$\\operatorname{Gumbel Softmax}$，回到主题上，利用$\\\\text{Gumbel-Max Trick}$，对得分向量$\\mathbf s$中每个元素加上独立$\\operatorname{Gumbel}$噪声，使得:')\n",
      "st.latex(r'''\\tilde {\\mathbf s}=\\beta\\log\\mathbf s_i+g_i,g_i\\sim \\operatorname{Gumbel}(0,1)''')\n",
      "st.markdown('​然后对得到的$\\tilde {\\mathbf s}$进行排序，对应的置换向量为$\\tilde {\\mathbf z}$，对应的置换矩阵就是$P_{\\operatorname{\\tilde {\\mathbf s}}}$，和确定性排序一样，利用$\\operatorname{softmax with temperature}$近似，即：')\n",
      "st.latex(r'''\\lim _{\\tau \\rightarrow 0^{+}} \\widehat{P}_{\\operatorname{sort}(\\tilde {\\mathbf{s}})}[i,:](\\tau)=P_{\\operatorname{sort}(\\tilde {\\mathbf{s}}))}[i,:] \\quad \\forall i \\in\\{1,2, \\ldots, n\\}''')\n",
      "st.markdown('​因此，重参数化后的优化目标可以表示如下：')\n",
      "st.latex(r'''\\begin{aligned}\\mathcal L(\\theta,\\mathbf s)=\\mathbb E_{\\mathbf g\\sim \\operatorname{Gumbel}(0,1)}\\big[f(\\hat P_{\\operatorname{sort}(\\beta\\log \\mathbf s+\\mathbf g)};\\theta)\\big]\\\\\\nabla_{\\mathbf s}\\mathcal L(\\theta,\\mathbf s)=\\mathbb E_{\\mathbf g\\sim \\operatorname{Gumbel}(0,1)}\\big[\\nabla_{\\mathbf s}f(\\hat P_{\\operatorname{sort}(\\beta\\log \\mathbf s+\\mathbf g)};\\theta)\\big]\\end{aligned}''')\n",
      "st.markdown('​接下来结合$\\operatorname{kNN}$的例子加强对该算法的理解，我们先回顾一下原始的$\\operatorname{kNN}$，$\\operatorname{kNN}$是一种 **非参数（non-parametric）监督学习算法**，既可以用于分类，也可以用于回归。其核心思想非常简单：')\n",
      "st.markdown('​假设一个分类任务，给定训练样本和对应的标签$\\mathcal D=\\{(\\mathbf x_1,y_1),(\\mathbf x_2,y_2),...,(\\mathbf x_n,y_n)\\}$，要预测新的样本$\\mathbf x_{\\\\text{new}}$属于哪一个类别，那么只需判断该新样本距离最近的$k$个（$k$是超参数）距离$d$对应的训练数据的类别，再透过投票来决定该样本所述的类别即可。距离$d$可以是欧氏距离、余弦相似度、曼哈顿距离等。具体地，选取一个查询样本$(\\mathbf {x_0},y_0)$，随机选取$n$个样本作为其候选邻居${(\\mathbf x_1,y_1),\\ldots}$,${(\\mathbf x_n,y_n)}$，然后神经网络会学到一个映射表示$h_{\\phi}(\\cdot)$，将输入编码成高维语义向量，即$h_{\\phi}(\\mathbf x_i)^{\\\\top}\\in\\mathbb R^{h\\times 1}$，然后计算查询样本和候选样本在语义空间中的度量：')\n",
      "st.latex(r'''\\mathbf s_j=|| h_{\\phi}(\\mathbf x_0) -h_{\\phi}(\\mathbf x_j)||^2_2,j=1,\\ldots,n''')\n",
      "st.markdown('​得到了分数向量便可以采用$\\operatorname{NeuralSort}$的松弛排序$\\hat P_{\\operatorname{sort}(\\mathbf s)}$，而策略的选择，即$f$可以用如下公式：')\n",
      "st.latex(r'''\\mathcal l_{\\operatorname{kNN}}(\\hat P_{\\operatorname{sort}(\\mathbf s)},y_0,\\ldots,y_n)==-\\frac{1}{k} \\sum_{j=1}^{k} \\sum_{i=1}^{n} 1\\left(y_{i}=y_{0}\\right) \\hat{P}_{z}[i, j]''')\n",
      "st.markdown('​本质上就是交叉熵损失。即按照列的方向扫过去，判断前$k$列预测的结果和真实结果是否一致。在推理时，给定测试样本$\\mathbf x'_0$，先计算对应的语义表征$\\mathbf e_0=h_{\\phi}(\\mathbf x_0')$,计算训练集中所有点的语义表征$e_j=h_{\\phi}(\\mathbf x_j),j=1,\\ldots,|\\mathcal D|$，然后用欧式距离排序，选择$k$​个最近邻居，通过多数投票确定预测标签。原论文还有手写数字识别数据集和分位回归任务的实验，理解了上文的讲解便能触类旁通，笔者就不在此展开介绍了。')\n",
      "st.markdown('​上文中$2.3$提到$\\\\text{NDCG}$指标计算依赖于$\\\\text{DCG}$和$\\\\text{IDCG}$，$\\\\text{DCG}$中涉及到了$\\mathrm {sort}$的操作是不可导算子，导致神经网络无法优化，那么现在有了$\\operatorname{NeuralSort}$便可以自然地想到将可导的置换矩阵用于近似排序算子从而优化$\\operatorname{NDCG@k}$，回顾一下公式：')\n",
      "st.latex(r'''\\begin{aligned} \\text{NDCG@}k(\\mathbf s,\\mathbf r)&=\\frac{\\text{DCG@}k(\\mathbf s,\\mathbf r)}{\\text{IDCG@}k(\\mathbf r)}\\\\\\text{DCG@}k(\\mathbf s,\\mathbf r)&=\\sum_{j=1}^{k}g(r_{\\pi^{-1}(j)})D(j)\\\\\\text{IDCG@}k(\\mathbf r)&=\\sum_{j=1}^{k}g(r_{\\pi_\\mathbf r^{-1}(j)})D(j)=\\max \\text{DCG@}k(\\mathbf s,\\mathbf r) \\end{aligned}''')\n",
      "st.markdown('​$\\\\text{IDCG}@k$就是固定的，可以提前计算好。而$\\operatorname{DCG@}k$的计算要依赖于神经网络输出的文档相关性分数向量$\\mathbf s$排序，得到排序后的分数向量中对应的$r_i$。给定神经网络的输出分数向量$\\mathbf s$，$\\mathbf s$对应于一个置换矩阵$P_{\\operatorname{sort}(\\mathbf s)}$，给$\\mathbf s$排序等价于置换矩阵右乘以$\\mathbf s$即$P_{\\operatorname{sort}(s)}\\mathbf s$，为了可导的性质，我们实际上用一个单峰行随机矩阵$\\hat P_{\\operatorname{sort}(\\mathbf s)}(\\tau)$来近似真正的$P_{\\operatorname{sort}(s)}$（$\\operatorname{NeuralSort}$）。')\n",
      "st.latex(r'''\\lim _{\\tau \\rightarrow 0^{+}} \\widehat{P}_{\\operatorname{sort}(\\mathbf{s})}[i,:](\\tau)=\\operatorname{softmax}\\left[\\left((n+1-2 i) \\mathbf s-A_{\\mathbf s} \\mathbb{1}\\right) / \\tau\\right] \\quad \\forall i \\in\\{1,2, \\ldots, n\\}''')\n",
      "st.markdown('​$\\hat P_{\\operatorname{sort}(s)}(\\tau)$在下文记作。为计算$\\operatorname{DCG@}k$，我们要知道按照$\\mathbf s$排序后对应的相关性分数标签$r_i$和增益$g_i$，因此有$\\hat P g(\\mathbf r)$，意思是相关性标签列表按照$\\mathbf s$大小进行排序对应的增益。因此最原始情况下的$\\operatorname{NeuralNDCG}@k(\\tau)(\\mathbf {s,r})$公式可以表达如下：')\n",
      "st.latex(r'''\\begin{aligned} \\operatorname{NeuralNDCG}@k(\\tau)(\\mathbf {s,r})=\\frac{\\sum_{i=1}^{k}\\big(\\hat P g(\\mathbf r)\\big)_id(i)}{\\operatorname{IDCG@}k}\\end{aligned}''')\n",
      "st.markdown('​$\\hat P$矩阵的性质是每一行之和为$1$，每一列之和不一定为$1$，这样的性质会有什么影响呢？我们可以分别看公式：$\\\\begin{aligned}\\sum_{i=1}^{k}P g(\\mathbf r)\\\\end{aligned}$与$\\\\begin{aligned}\\sum_{i=1}^{k}\\hat P g(\\mathbf r)\\\\end{aligned}$。前者是文档真实的获得的增益有：')\n",
      "st.latex(r'''\\begin{aligned} \\end{aligned}\\begin{aligned}\\sum_{i=1}^{k}(P g(\\mathbf r))_i=\\sum_{i=1}^{k}\\sum_{j=1}^{k}P[i,j]g(\\mathbf r)_j=\\sum_{i=1}^{k}g(r_i)\\end{aligned}''')\n",
      "st.markdown('​后者为：')\n",
      "st.latex(r'''\\begin{aligned} \\end{aligned}\\begin{aligned}\\sum_{i=1}^{k}(\\hat P g(\\mathbf r))_i&=\\sum_{i=1}^{k}\\sum_{j=1}^{k} \\hat P[i,j]g(\\mathbf r)_j\\\\&=\\sum_{j=1}^{k}g(r_j)\\sum_{i=1}^{k}\\hat P[i,j]\\\\&\\neq \\sum_{i=1}^{k}g(r_i)\\end{aligned}''')\n",
      "st.markdown('​也就是说近似的置换矩阵每一列之和不为$1$会使得最终计算的增益要么变大要么变小，即某个文档的增益在排名中可能超量也可能少量，导致$\\operatorname{NDCG}$指标的计算偏离预期。可以通过$\\operatorname {Sinkhorn Scaling}$把$\\hat P$进行行列归一化，确保所有文档对排名的贡献为$1$​。具体步骤如下：')\n",
      "st.markdown('​故改进后的$\\operatorname{NeuralNDCG}@k(\\tau)(\\mathbf {s,r})$公式为：')\n",
      "st.latex(r'''\\begin{aligned} \\operatorname{NeuralNDCG}@k(\\tau)(\\mathbf {s,r})=\\frac{\\sum_{i=1}^{k}\\big(S g(\\mathbf r)\\big)_id(i)}{\\operatorname{IDCG@}k}\\end{aligned}''')\n",
      "st.markdown('# 4.Ranking skills in Direct Preference Optimization')\n",
      "st.markdown('​预训练——有监督微调——人类反馈强化学习是打造一个高性能大语言模型的标准步骤。在对齐阶段，目前的RLHF技术如PPO在训练时不够稳定，且对计算资源要求高，为此，$\\\\text{DPO}$技术应运而生，$\\\\text{DPO}$的思想是将$\\\\text{RLHF}$中显示的奖励函数转化到统一的有监督损失中，使得模型可以通过有监督的方式微调参数，在给定偏好数据（人类专家对同一输入不同输出的优劣判断）的情况下，直接学习生成更优的输出，从而绕过传统$\\mathrm{RLHF}$​中复杂且不稳定的策略优化过程。')\n",
      "st.markdown('​在实际工作中，大部分的时间都是与收集与清洗数据的工作打交道，高质量的数据对提升模型下游任务性能有最直接的影响，且模型与策略层面的改动相对来说较少。因此，本文将从数据策略的维度介绍DPO，并从Learning to Rank 的视角解析$\\\\text{DPO}$。站在数据策略的角度，$\\\\text{DPO}$可以分成数据质量(Data Quality)、偏好反馈(Preference Feedback)、偏好细粒度(Preference Granularity)三个层面[[12]](https://arxiv.org/abs/2503.11701)，本文将先介绍偏好反馈与信息检索领域的技术关联。')\n",
      "st.markdown('## 4.1 Preference Feedback')\n",
      "st.markdown('​偏好反馈指的可以分为$\\mathrm{PointWise}$反馈、$\\mathrm{PairWise}$反馈和$\\mathrm{ListWise}$反馈三类。与$\\\\text{Ranking}$问题一样，$\\mathrm{PointWise}$反馈独立评估每个回答的好坏，往往视作一个回归或分类问题，为其打分或标注为正面或负面；$\\mathrm{PairWise}$反馈通过构造成对的偏序关系比较两两之间的好坏；而$\\mathrm{ListWise}$反馈则考虑了整个文档内的好坏关系，本文着重介绍成对反馈与列表级反馈。')\n",
      "st.markdown('### 4.1.1 Pair-Wise Feedback')\n",
      "st.markdown('​成对反馈侧重于比较成对问答的偏序关系，即给定上下文历史$x_i$，和不同的回复$y^{(i)}_j$,$y^{(i)}_k$。判断回答的相对好坏，即$(x_i,y^{(i)}_j)?(x_i,y^{(i)}_k)$。**Rafailov**&**Sharma**[[13]](https://arxiv.org/abs/2305.18290)等人提出的DPO用$\\\\text{Bradley-Terry}$模型建模偏好的回复$y_1$大于另一个回复的$y_2$概率，即：')\n",
      "st.latex(r'''\\begin{aligned}p^*(y_1\\succ y_2)&=\\sigma(r(x,y_1)-r(x,y_2))\\\\&=\\frac{1}{1+\\exp{\\bigg(\\beta\\log{\\frac{\\pi^*(y_2|x)}{\\pi_{\\text{ref}}(y_2|x)}}-\\beta\\log{\\frac{\\pi^*(y_1|x)}{\\pi_{\\text{ref}}(y_1|x)}}\\bigg)}}\\end{aligned}''')\n",
      "st.markdown('​借助极大似然估计的思想$\\\\begin{aligned}\\\\arg\\max_{\\\\theta}\\log P(X;\\\\theta)\\\\end{aligned}$，优化目标$\\mathcal L$可以写成如下形式：')\n",
      "st.latex(r'''\\mathcal L_{\\mathrm{DPO}}(\\pi_{\\theta};\\pi_{\\text{ref}})=-\\mathbb E_{(x,y_c,y_r)\\sim \\mathcal D}\\big[\\log\\sigma\\big(\\beta\\log{\\frac{\\pi^*(y_c|x)}{\\pi_{\\text{ref}}(y_c|x)}}-\\beta\\log{\\frac{\\pi^*(y_r|x)}{\\pi_{\\text{ref}}(y_r|x)}}\\big)\\big]''')\n",
      "st.markdown('​其实这个损失函数主体形式和$\\mathrm{RankNet}$的损失函数一样：')\n",
      "st.latex(r'''\\begin{aligned} \\mathcal L_{\\mathrm{RankNet}}&=-\\mathbb E_{(x,y_i,y_j)\\sim \\mathcal D}\\big[\\log \\left(1+e^{-\\beta\\left(s_{i}-s_{j}\\right)}\\right)\\big]\\\\&=-\\mathbb E_{(x,y_i,y_j)\\sim \\mathcal D}\\big[\\log \\sigma (\\beta s_i-\\beta s_j)\\big]\\end{aligned}''')\n",
      "st.markdown('​其中,$\\sigma$是$\\mathrm{sigmoid}$函数，可以看到，二者只是变量不同，$\\mathrm{RankNet}$中的$s_i$对应了$\\mathrm{DPO}$中的$\\log{\\\\frac{\\pi^*(y_c|x)}{\\pi_{\\\\text{ref}}(y_c|x)}}$，$s_j$对应$\\log{\\\\frac{\\pi^*(y_r|x)}{\\pi_{\\\\text{ref}}(y_r|x)}}$。因此$\\mathrm{DPO}$的训练过程可以视作是$\\\\text{Learning to Rank}$,给定上下文$x_i$，偏好的回复$y_c$和次之的回复$y_r$，DPO是在学习让$y_c$中的$\\\\text{token}$的概率排到$y_r$更前面的位置。此外，$DPO$的损失函数可以不只是借助$\\\\text{Bradley Terry}$模型的概率建模，还可以将信息检索领域的$\\mathrm{PairWise}$损失集成，如上文中所介绍的$\\mathrm{LambdaRank}$。')\n",
      "st.markdown('​$\\mathrm{DPO}$在训练时会出现正负例奖励同时上升或者下降的情况[[14]](https://zhuanlan.zhihu.com/p/1907949654739513685)。是因为其损失函数只需要正例输出的相对概率比负例大（强调二者间的相对关系，强化二者间的差值，而非绝对大小。），比如$\\\\frac{0.35}{0.12}-\\\\frac{0.11}{0.12}$和$\\\\frac{0.29}{0.11}-\\\\frac{0.10}{0.11}$都符合正例的相对概率比负例大，但实际上正例的奖励降低了，即存在多种正负例取值的可能满足损失函数在减小但是正负例的奖励增大或者减小。笔者在此给出定量的梯度更新分析，记：')\n",
      "st.latex(r'''\\begin{aligned} A=\\beta\\log{\\frac{\\pi^*(y_c|x)}{\\pi_{\\text{ref}}(y_c|x)}},B=\\beta\\log{\\frac{\\pi^*(y_r|x)}{\\pi_{\\text{ref}}(y_r|x)}}\\end{aligned}''')\n",
      "st.markdown('​求损失函数$\\mathcal L=-\\log(\\sigma(A-B))$对参数分量$w_k$的梯度：')\n",
      "st.latex(r'''\\begin{aligned}\\frac{\\partial \\mathcal L}{\\partial w_k}&=\\frac{\\partial \\mathcal L}{\\partial A}\\frac{\\partial A}{\\partial w_k}+\\frac{\\partial \\mathcal L}{\\partial B}\\frac{\\partial B}{\\partial w_k}\\\\&=\\frac{\\sigma(A-B)*(1-\\sigma(A-B))}{\\sigma(A-B)}*\\frac{\\partial A}{\\partial w_k}-\\frac{\\sigma(A-B)*(1-\\sigma(A-B))}{\\sigma(A-B)}*\\frac{\\partial B}{\\partial w_k}\\\\&=(1-\\sigma(A-B))*\\beta(\\frac{\\pi_{\\text{ref}}(y_c|x)}{\\pi^*(y_c|x)}*\\frac{\\partial\\pi^*(y_c|x)}{\\partial w_k}-\\frac{\\pi_{\\text{ref}}(y_r|x)}{\\pi^*(y_r|x)}*\\frac{\\partial\\pi^*(y_r|x)}{\\partial w_k})\\end{aligned}''')\n",
      "st.markdown('​因此从梯度的角度看，rejected reward的梯度是占据主导地位的，由于损失函数的设计使得模型在优化时无法直接提升chosen reward，因此rejected reward若迅速降低，chosen reward存在不提升，但是慢慢降低的情况，此使使得模型输出的chosen reward仍然大于rejected reward，但是chosen reward的降低会使得模型在训练时逐渐变得不再输出人类偏好的token，训练完的模型会胡说八道，因此在训练过程中需要调整超参数或者引入额外损失等手段解决这个问题，如引入有监督阶段的SFT损失函数，提升模型输出chosen token的概率（DeepSpeed-Chat的RLHF阶段在ppo过程中可以选择性添加预训练阶段任务，即一边ppo让模型的收益增大，一边防止模型能力跑偏，因此在DPO时引入SFT的损失也是可行的手段之一）。')\n",
      "st.markdown('​DPO运用了Bradley Terry模型建模不同偏好回复的胜负概率，当成对标注出现$y_j=y_k$时，$\\\\text{Bradley Terry}$模型无法准确建模，成对比较出现打平的情况是十分常见的，如在CBT-Bench中可以看到不同模型和参考答案比较时二者打平的情况其实不在少数，那么如何解决成对比较打平的问题?——引入新的比较模型或是借鉴Learing to Rank的策略。前者的方式是将Bradley Terry 模型替换成可以建模平均概率的$\\\\text{Rao-Kupper }$模型与$\\\\text{Davidson}$模型[[15]](https://arxiv.org/pdf/2409.17431)。本文仅以$\\\\text{Rao-Kupper }$模型为例:')\n",
      "st.latex(r'''\\begin{aligned}p(y_i\\succ y_j)&=\\frac{\\lambda_i}{\\lambda_i+\\mathcal V\\lambda_j}=\\frac{1}{1+\\mathcal V\\lambda_j/\\lambda_i}\\\\p(y_i\\sim y_j)&=\\frac{(\\mathcal V^2-1)\\lambda_i\\lambda_j}{(\\lambda_i+\\mathcal V\\lambda_j)(\\lambda_j+\\mathcal V\\lambda_i)}=\\frac{(\\mathcal V^2-1)}{(1+\\mathcal V\\lambda_j/\\lambda_i)(1+\\mathcal V\\lambda_i/\\lambda_j)}\\end{aligned}''')\n",
      "st.markdown('​此时有$p(y_i\\succ y_j)+p(y_j\\succ y_i)+p(y_i\\sim y_j)=1$，是合法的概率密度函数，其中$\\mathcal V$用于控制模型分配给平均的概率，我们可以看看$p(y_i\\succ y_j)+p(y_j\\succ y_i)$与$p(y_i \\sim y_j)$的关系，当$\\lambda_i=\\lambda_j$时：')\n",
      "st.latex(r'''\\begin{aligned} p(y_i\\succ y_j)+p(y_j\\succ y_i)&=\\frac{\\lambda_i}{\\lambda_i+\\mathcal V\\lambda_j}+\\frac{\\lambda_j}{\\lambda_j+\\mathcal V\\lambda_i}\\\\&=\\frac{2\\lambda_i\\lambda_j+\\mathcal V(\\lambda_i^2+\\lambda_j^2)}{(\\lambda_i+\\mathcal V\\lambda_j)(\\lambda_j+\\mathcal V\\lambda_i)}\\\\&=\\frac{2(\\mathcal V+1)\\lambda^2}{(1+\\mathcal V)^2\\lambda^2}\\\\&=\\frac{\\mathcal V-1}{2}p(y_i\\sim y_j)\\end{aligned}''')\n",
      "st.markdown('​这表明参数$\\mathcal V$决定了匹配的项目被判定为平局或不平局的概率，取$\\mathcal V=3$有平局概率与不平局概率相同，皆为$0.5$。记$\\lambda_i$为$\\log{\\\\frac{\\pi^*(y_c|x)}{\\pi_{\\\\text{ref}}(y_c|x)}}$,$\\lambda_j$为$\\log{\\\\frac{\\pi^*(y_r|x)}{\\pi_{\\\\text{ref}}(y_r|x)}}$。那么此时，基于$\\\\text{Rao-Kupper }$模型的损失函数可以写作：')\n",
      "st.latex(r'''\\begin{aligned}\\mathcal L_{\\mathrm{DPO}^{\\mathrm{RK}}}(\\pi_{\\theta};\\pi_{\\text{ref}})&=-\\mathbb E_{(x,y_c\\succ y_r)\\sim \\mathcal D}\\big[\\log\\sigma(\\lambda_i-\\lambda_j-\\alpha)\\big]-\\\\ &\\mathbb E_{(x,y_c\\sim y_r)\\sim \\mathcal D}\\big[\\log\\sigma(\\lambda_j-\\lambda_i-\\alpha)+\\log(\\sigma(\\lambda_i-\\lambda_j-\\alpha))-\\log(\\mathcal V^2-1)\\big] \\end{aligned}''')\n",
      "st.markdown('### 4.1.2 List-Wise Feedback')\n",
      "st.markdown('​列表级反馈将考虑多个候选回答间的整体关系，即多个回答间的相对顺序。上一小节我们发现了DPO实际上可以看作是在排序学习，那么，在List-Wise FeedBack中我们同样可以引入传统的learning to rank策略，如像$\\mathrm{ListMLE}$一样直接优化排序出现的似然，即给定模型预测的分数向量$\\mathbf s$，按照$\\mathbf s$排序得到的置换向量是$\\pi$排序后得到$\\mathbf s_{\\pi}$，$s_{\\pi(k)}$是$\\mathbf s_{\\pi}$的第$k$个分量，也是第$k$大的元素，那么损失函数可以写作：')\n",
      "st.latex(r'''\\begin{aligned} \\mathcal L_{\\mathrm{ListMLE}}=-\\mathbb E_{x,\\mathbf y\\sim\\mathcal D}\\big[\\log\\prod_{k=1}^{K}\\frac{\\exp{s_{\\pi(k)}}}{\\sum_{j=k}^{K}\\exp(s_{\\pi(k)})}\\big]\\end{aligned}''')\n",
      "st.markdown('​同样，$\\mathrm{ListNet}$损失函数也能用于$\\mathrm{DPO}$，给定真实的相关性标签$\\mathbf r=(r_1,\\ldots,r_K)$，模型预测的分数向量$\\mathbf s$，损失函数可以写成：')\n",
      "st.latex(r'''\\begin{aligned} \\mathcal L_{\\mathrm{ListNet}}&=-\\mathbb E_{x,\\mathbf y,\\psi\\sim\\mathcal D}\\big[\\sum_{k=1}^{K}P_{r_k}\\log\\sum_{k=1}^{K}\\frac{\\exp(s_k)}{\\sum_{j=k}^{K}\\exp(s_j)}\\big]\\\\&=-\\mathbb E_{x,\\mathbf y,\\psi\\sim\\mathcal D}\\big[\\sum_{k=1}^{K}\\frac{r_k}{\\sum_{j=1}^{K}r_j}\\log\\bigg(\\sum_{k=1}^{K}\\frac{\\exp(s_k)}{\\sum_{j=1}^{K}\\exp(s_j)}\\bigg)\\big]\\end{aligned}''')\n",
      "st.markdown('​$\\\\text{LiPO-}\\lambda$[[16]](https://arxiv.org/pdf/2402.01878)直接定义$\\lambda$梯度，得到优化目标如下：')\n",
      "st.latex(r'''\\mathbb{E}_{x, \\mathbf{y}, \\psi \\sim \\mathcal{D}}\\left[\\sum_{\\psi_{i}>\\psi_{j}} \\Delta_{i, j} \\log \\left(1+e^{-\\left(s_{i}-s_{j}\\right)}\\right)\\right],''')\n",
      "st.markdown('​其中$\\Delta_{ij}$公式如下：')\n",
      "st.latex(r'''\\begin{aligned}\\Delta_{i, j}=\\left|G_{i}-G_{j}\\right| \\cdot\\left|\\frac{1}{D(\\pi(i))}-\\frac{1}{D(\\pi(j))}\\right|\\end{aligned}''')\n",
      "st.markdown('​$G_i$是文档的增益，$D(\\pi(i))=\\log(1+\\pi(i))$是折扣因子，$\\pi(i)$是按照分数$\\mathbf s$排序后文档$y_i$的位置。至此，不难发现$\\\\text{DPO}$可以无缝融入各种各样的排序损失 ，上文第三章中提到的$\\mathrm{NeuralNDCG}$同样也可以用于$\\\\text{DPO}$，zhao等人提出的$\\mathrm{OPO}$[[17]](https://arxiv.org/pdf/2410.04346)便是基于这个思想，借用公式(3-x)，$\\mathrm{OPO}$的优化目标可以写成：')\n",
      "st.latex(r'''\\begin{aligned} \\begin{aligned} \\operatorname{NeuralNDCG}@k(\\tau)(\\mathbf {s,r})=\\mathbb E_{x,\\mathbf y,\\mathbf r\\sim \\mathcal D}\\bigg[\\frac{\\sum_{i=1}^{k}\\big(S g(\\mathbf r)\\big)_id(i)}{\\operatorname{IDCG@}k}\\bigg]\\end{aligned}\\end{aligned}''')\n",
      "st.markdown('​$\\mathrm{OPO}$列举了不同反馈标注形式下的工作、类型及优化目标，如下图：')\n",
      "st.image('assets/image-20250709143803491.png')\n",
      "st.markdown('​此外，$\\mathrm{OPO}$基于$\\\\text{UltraFeedback}$和$\\\\text{SimPO}$构建了一个有序奖励的数据集，并通过实验结果表明使用多样化的负样本比仅使用最低质量的回答作为负样本更能够提升模型性能。')\n",
      "st.markdown('# 5.参考文献')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393be62b-9253-4a48-b1d0-c0a76159863d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e0a65-4631-4009-b8f9-bbee488579d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc77c08-47e9-41e9-86d8-a17036933d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a81657-a93a-4519-9fe6-2eb36327a81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
